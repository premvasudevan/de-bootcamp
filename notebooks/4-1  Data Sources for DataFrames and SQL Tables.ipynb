{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188a530e-de97-440c-8cb9-7822487bda07",
   "metadata": {},
   "source": [
    "# Data Sources for DataFrames and SQL Tables\n",
    "As shown in Figure 4-1, Spark SQL provides an interface to a variety of data sources.\n",
    "It also provides a set of common methods for reading and writing data to and from\n",
    "these data sources using the Data Sources API.\n",
    "In this section we will cover some of the built-in data sources, available file formats,\n",
    "and ways to load and write data, along with specific options pertaining to these data\n",
    "sources. But first, let’s take a closer look at two high-level Data Source API constructs\n",
    "that dictate the manner in which you interact with different data sources: DataFrameR\n",
    "eader and DataFrameWriter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5fe02-9099-4671-876d-9565a4eef216",
   "metadata": {},
   "source": [
    "## DataFrameReader\n",
    "DataFrameReader is the core construct for reading data from a data source into a\n",
    "DataFrame. It has a defined format and a recommended pattern for usage:\n",
    "\n",
    "DataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()\n",
    "\n",
    "This pattern of stringing methods together is common in Spark, and easy to read. We\n",
    "saw it in Chapter 3 when exploring common data analysis patterns.\n",
    "Note that you can only access a DataFrameReader through a SparkSession instance.\n",
    "That is, you cannot create an instance of DataFrameReader. To get an instance handle\n",
    "to it, use:\n",
    "\n",
    "SparkSession.read\n",
    "// or\n",
    "SparkSession.readStream\n",
    "\n",
    "While read returns a handle to DataFrameReader to read into a DataFrame from a\n",
    "static data source, readStream returns an instance to read from a streaming source.\n",
    "(We will cover Structured Streaming later in the book.)\n",
    "\n",
    "Arguments to each of the public methods to DataFrameReader take different values.\n",
    "Table 4-1 enumerates these, with a subset of the supported arguments.\n",
    "\n",
    "While we won’t comprehensively enumerate all the different combinations of arguments\n",
    "and options, the documentation for Python, Scala, R, and Java offers suggestions\n",
    "and guidance. It’s worthwhile to show a couple of examples, though:\n",
    "\n",
    "// In Scala\n",
    "// Use Parquet\n",
    "val file = \"\"\"/databricks-datasets/learning-spark-v2/flights/summarydata/\n",
    "parquet/2010-summary.parquet\"\"\"\n",
    "val df = spark.read.format(\"parquet\").load(file)\n",
    "\n",
    "// Use Parquet; you can omit format(\"parquet\") if you wish as it's the default\n",
    "val df2 = spark.read.load(file)\n",
    "\n",
    "// Use CSV\n",
    "val df3 = spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"mode\", \"PERMISSIVE\")\n",
    ".load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\")\n",
    "\n",
    "// Use JSON\n",
    "val df4 = spark.read.format(\"json\")\n",
    ".load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f38f3-dbc6-472d-b66f-908755550ca2",
   "metadata": {},
   "source": [
    "# DataFrameWriter\n",
    "DataFrameWriter does the reverse of its counterpart: it saves or writes data to a specified\n",
    "built-in data source. Unlike with DataFrameReader, you access its instance not\n",
    "from a SparkSession but from the DataFrame you wish to save. It has a few recommended\n",
    "usage patterns:\n",
    "\n",
    "DataFrameWriter.format(args)\n",
    ".option(args)\n",
    ".bucketBy(args)\n",
    ".partitionBy(args)\n",
    ".save(path)\n",
    "\n",
    "DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)\n",
    "\n",
    "To get an instance handle, use:\n",
    "\n",
    "DataFrame.write\n",
    "// or\n",
    "DataFrame.writeStream\n",
    "\n",
    "Arguments to each of the methods to DataFrameWriter also take different values. We\n",
    "list these in Table 4-2, with a subset of the supported arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b04c9-08da-4710-8a42-bc90a04a1e3f",
   "metadata": {},
   "source": [
    "### rest of the topics in the book is related to reading/writing different data sources such as parquet(default), json, csv, avro, orc using dataframe and spark sql. please refer docs for more info and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c96c4b-800c-425c-b71f-a1ec2f36c062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ea919-0a44-4872-b874-9e01e926a5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498ae5f-b8cc-42e4-9a53-1bd1bf26eeea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2d595-a489-4351-bd13-a67ace8dbf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd19bf-b616-41dd-9e17-89b30dbcf2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be959ad9-62b6-4026-8ee2-8cf9dc38451c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
