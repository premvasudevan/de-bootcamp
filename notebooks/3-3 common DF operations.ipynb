{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121242f6-9f94-4387-a845-bddb5ff6d616",
   "metadata": {},
   "source": [
    "# common dataframe operations\n",
    "performed on san francisco fire call dataset\n",
    "\n",
    "\n",
    "# questions\n",
    "1. diff b/w spark.read spark.DataFrameReader, similarly df.write and dataframewriter\n",
    "\n",
    "\n",
    "# notes\n",
    "Spark provides an interface, DataFrameReader, that enables you to read data into a DataFrame from myriad\n",
    "data sources in formats such as JSON, CSV, Parquet, Text, Avro, ORC, etc. Likewise,\n",
    "to write a DataFrame back to a data source in a particular format, Spark uses DataFrameWriter\n",
    "\n",
    "To write the DataFrame into an external data source in your format of choice, you\n",
    "can use the DataFrameWriter interface. Like DataFrameReader, it supports multiple\n",
    "data sources. Parquet, a popular columnar format, is the default format; it uses\n",
    "snappy compression to compress the data. If the DataFrame is written as Parquet, the\n",
    "schema is preserved as part of the Parquet metadata. In this case, subsequent reads\n",
    "back into a DataFrame do not require you to manually supply a schema\n",
    "\n",
    "\n",
    "# errors faced and solutions\n",
    "https://stackoverflow.com/questions/49102292/file-already-exists-error-writing-new-files-from-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58f405bd-0d36-48bb-8bb5-c4923607578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/26 19:38:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# imports and initialisation\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrameReader, DataFrameWriter\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), \"datasets\", \"Fire_Department_Calls_for_Service.csv\")\n",
    "\n",
    "# spark = SparkSession.builder\\\n",
    "#                     .appName(\"SF_DF\")\\\n",
    "#                     .getOrCreate()\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/52133731/how-to-solve-cant-assign-requested-address-service-sparkdriver-failed-after\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"SF_DF\")\\\n",
    "                    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb789fa-54f3-4154-b377-4a334b599f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema\n",
    "\n",
    "sf_firecall_schema = StructType([\n",
    "    StructField('CallNumber', IntegerType(), True),\n",
    "    StructField('UnitID', StringType(), True),\n",
    "    StructField('IncidentNumber', IntegerType(), True),\n",
    "    StructField('CallType', StringType(), True),\n",
    "    StructField('CallDate', StringType(), True),\n",
    "    StructField('WatchDate', StringType(), True),\n",
    "    StructField('ReceivedDtTm', StringType(), True),\n",
    "    StructField('EntryDtTm', StringType(), True),\n",
    "    StructField('DispatchDtTm', StringType(), True),\n",
    "    StructField('ResponseDtTm', StringType(), True),\n",
    "    StructField('OnSceneDtTm', StringType(), True),\n",
    "    StructField('TransportDtTm', StringType(), True),\n",
    "    StructField('HospitalDtTm', StringType(), True),\n",
    "    StructField('CallFinalDisposition', StringType(), True),\n",
    "    StructField('AvailableDtTm', StringType(), True),\n",
    "    StructField('Address', StringType(), True),\n",
    "    StructField('City', StringType(), True),\n",
    "    StructField('ZipcodeofIncident', IntegerType(), True),\n",
    "    StructField('Battalion', StringType(), True),\n",
    "    StructField('StationArea', StringType(), True),\n",
    "    StructField('Box', StringType(), True),\n",
    "    StructField('OriginalPriority', StringType(), True),\n",
    "    StructField('Priority', StringType(), True),\n",
    "    StructField('FinalPriority', StringType(), True),\n",
    "    StructField('ALSUnit', BooleanType(), True),\n",
    "    StructField('CallTypeGroup', StringType(), True),\n",
    "    StructField('NumberofAlarms', IntegerType(), True),\n",
    "    StructField('UnitType', StringType(), True),\n",
    "    StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "    StructField('FirePreventionDistrict', StringType(), True),\n",
    "    StructField('SupervisorDistrict', StringType(), True),\n",
    "    StructField('NeighborhooodsAnalysisBoundaries', StringType(), True),\n",
    "    StructField('Location', StringType(), True),\n",
    "    StructField('RowID', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ac57e8-f90c-4d63-a9c2-5ce901a5ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_firecall_df = spark.read.csv(dataset_path, sf_firecall_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d377581a-a62f-4de7-8dbd-858d9c18f570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Number, Call Type, Call Date, City\n",
      " Schema: CallNumber, CallType, CallDate, City\n",
      "Expected: CallNumber but found: Call Number\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------------------+----+\n",
      "|CallNumber|CallDate  |CallType                     |City|\n",
      "+----------+----------+-----------------------------+----+\n",
      "|1030107   |04/12/2000|Alarms                       |SF  |\n",
      "|1030112   |04/12/2000|Citizen Assist / Service Call|SF  |\n",
      "|1030116   |04/12/2000|Electrical Hazard            |SF  |\n",
      "|1030117   |04/12/2000|Odor (Strange / Unknown)     |SF  |\n",
      "|1030120   |04/12/2000|Alarms                       |SF  |\n",
      "|1030128   |04/12/2000|Alarms                       |SF  |\n",
      "|1030128   |04/12/2000|Alarms                       |SF  |\n",
      "|1030132   |04/12/2000|Other                        |SF  |\n",
      "|1030136   |04/12/2000|Structure Fire               |SF  |\n",
      "|1030143   |04/12/2000|Other                        |SF  |\n",
      "+----------+----------+-----------------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf_firecall_df.select(\"CallNumber\",\"CallDate\",\"CallType\",\"City\").where(col(\"CallType\") != \"Medical Incident\").show(n=10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37710c75-6052-4cf4-93d2-f2f671096a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sf_firecall_df.write.format(\"parquet\").save(os.path.join(os.getcwd(), \"dst\",\"sf_firecall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2896929-daf7-454e-82b9-3ee2fc8a4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can save it as a table, which registers metadata with the Hive metastore\n",
    "\n",
    "# sf_firecall_df.write.format(\"parquet\").saveAsTable(\"sf_firecall\")\n",
    "\n",
    "# wonder where the table data has been stored ? \"spark-warehouse\" is created in cwd and inside folder named as table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e169599-0198-4eab-ab2a-0a4c21087cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Type\n",
      " Schema: CallType\n",
      "Expected: CallType but found: Call Type\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "[Stage 1:====================================================>    (13 + 1) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|DistinctCallTypes_count|\n",
      "+-----------------------+\n",
      "|                     32|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# distinct call types count\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "distinct_call_types_count = sf_firecall_df.\\\n",
    "                        where(col(\"CallType\").isNotNull()).\\\n",
    "                        agg(countDistinct(col(\"CallType\")).alias(\"DistinctCallTypes_count\"))\n",
    "\n",
    "distinct_call_types_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a24742-4876-4bac-b2d3-de46a59418d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Type\n",
      " Schema: CallType\n",
      "Expected: CallType but found: Call Type\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "[Stage 7:====================================================>    (13 + 1) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|CallType                                    |\n",
      "+--------------------------------------------+\n",
      "|Administrative                              |\n",
      "|Aircraft Emergency                          |\n",
      "|Alarms                                      |\n",
      "|Assist Police                               |\n",
      "|Citizen Assist / Service Call               |\n",
      "|Confined Space / Structure Collapse         |\n",
      "|Electrical Hazard                           |\n",
      "|Elevator / Escalator Rescue                 |\n",
      "|Explosion                                   |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|\n",
      "|Fuel Spill                                  |\n",
      "|Gas Leak (Natural and LP Gases)             |\n",
      "|HazMat                                      |\n",
      "|High Angle Rescue                           |\n",
      "|Industrial Accidents                        |\n",
      "|Lightning Strike (Investigation)            |\n",
      "|Marine Fire                                 |\n",
      "|Medical Incident                            |\n",
      "|Mutual Aid / Assist Outside Agency          |\n",
      "|Odor (Strange / Unknown)                    |\n",
      "+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# distinct call types list\n",
    "# from pyspark.sql.functions import \n",
    "\n",
    "distinct_call_types = sf_firecall_df.\\\n",
    "                        select(\"CallType\").\\\n",
    "                        where(col(\"CallType\").isNotNull()).\\\n",
    "                        distinct()\n",
    "\n",
    "distinct_call_types.sort(col(\"CallType\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893268ac-635c-4641-b4bc-104c6a844be8",
   "metadata": {},
   "source": [
    "# Renaming, adding, and dropping columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "771ad144-8e22-4e6b-b795-10ce1408875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a608b8-0ffe-4923-af90-920e2ee7c3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|NeighborhooodsBoundaries|\n",
      "+------------------------+\n",
      "|         Sunset/Parkside|\n",
      "|         Sunset/Parkside|\n",
      "|              Tenderloin|\n",
      "|              Tenderloin|\n",
      "|    Financial Distric...|\n",
      "+------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------+\n",
      "|NeighborhooodsBoundaries|\n",
      "+------------------------+\n",
      "|                Nob Hill|\n",
      "|                Nob Hill|\n",
      "|                    None|\n",
      "|               Chinatown|\n",
      "|                 Mission|\n",
      "|                 Mission|\n",
      "|                 Mission|\n",
      "|                Nob Hill|\n",
      "|                 Mission|\n",
      "|                Nob Hill|\n",
      "+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Neighborhooods - Analysis Boundaries\n",
      " Schema: NeighborhooodsAnalysisBoundaries\n",
      "Expected: NeighborhooodsAnalysisBoundaries but found: Neighborhooods - Analysis Boundaries\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "25/03/26 19:38:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Neighborhooods - Analysis Boundaries\n",
      " Schema: NeighborhooodsAnalysisBoundaries\n",
      "Expected: NeighborhooodsAnalysisBoundaries but found: Neighborhooods - Analysis Boundaries\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sf_firecall_df_renamed_col = sf_firecall_df\\\n",
    "                                .withColumnRenamed(\"NeighborhooodsAnalysisBoundaries\",\"NeighborhooodsBoundaries\")\n",
    "(sf_firecall_df_renamed_col.select(\"NeighborhooodsBoundaries\").show(n=5))\n",
    "(sf_firecall_df_renamed_col.select(\"NeighborhooodsBoundaries\").where(length(col(\"NeighborhooodsBoundaries\")) < 10).show(n=10))\n",
    "\n",
    "# there is also another method withColumnsRenamed to rename multiple cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e60a0383-09e2-42d5-bb97-c82beaf31c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4093c98-f2bd-427f-b80b-b2837ff1e9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('CallNumber', IntegerType(), True), StructField('UnitID', StringType(), True), StructField('IncidentNumber', IntegerType(), True), StructField('CallType', StringType(), True), StructField('CallDate', StringType(), True), StructField('WatchDate', StringType(), True), StructField('ReceivedDtTm', StringType(), True), StructField('EntryDtTm', StringType(), True), StructField('DispatchDtTm', StringType(), True), StructField('ResponseDtTm', StringType(), True), StructField('OnSceneDtTm', StringType(), True), StructField('TransportDtTm', StringType(), True), StructField('HospitalDtTm', StringType(), True), StructField('CallFinalDisposition', StringType(), True), StructField('AvailableDtTm', StringType(), True), StructField('Address', StringType(), True), StructField('City', StringType(), True), StructField('ZipcodeofIncident', IntegerType(), True), StructField('Battalion', StringType(), True), StructField('StationArea', StringType(), True), StructField('Box', StringType(), True), StructField('OriginalPriority', StringType(), True), StructField('Priority', StringType(), True), StructField('FinalPriority', StringType(), True), StructField('ALSUnit', BooleanType(), True), StructField('CallTypeGroup', StringType(), True), StructField('NumberofAlarms', IntegerType(), True), StructField('UnitType', StringType(), True), StructField('UnitSequenceInCallDispatch', IntegerType(), True), StructField('FirePreventionDistrict', StringType(), True), StructField('SupervisorDistrict', StringType(), True), StructField('NeighborhooodsBoundaries', StringType(), True), StructField('Location', StringType(), True), StructField('RowID', IntegerType(), True), StructField('response_in_days', StringType(), False)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding a new column\n",
    "# https://stackoverflow.com/questions/32788322/how-to-add-a-constant-column-in-a-spark-dataframe\n",
    "\n",
    "added_col = sf_firecall_df_renamed_col.withColumn(\"response_in_days\", lit(\"dummy\"))\n",
    "(added_col.describe)\n",
    "(added_col.schema)\n",
    "\n",
    "# currently this cell only outputs schema result which is last executed, to output both in jupyter cell configuration need to be changed\n",
    "# %config InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0df3765f-b68d-4ba8-a506-c4939214a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+------------------------+--------------------+-----+----------------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|        ReceivedDtTm|           EntryDtTm|        DispatchDtTm|        ResponseDtTm|         OnSceneDtTm|       TransportDtTm|        HospitalDtTm|CallFinalDisposition|       AvailableDtTm|             Address|City|ZipcodeofIncident|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumberofAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|NeighborhooodsBoundaries|            Location|RowID|response_in_days|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+------------------------+--------------------+-----+----------------+\n",
      "|   1030101|   E18|        306091|Medical Incident|04/12/2000|04/12/2000|04/12/2000 09:00:...|04/12/2000 09:01:...|04/12/2000 09:02:...|                NULL|                NULL|                NULL|                NULL|               Other|                NULL|2000 Block of 37T...|  SF|            94116|      B08|         18|0757|               3|       3|            3|  false|         NULL|             1|  ENGINE|                         1|                     8|                 4|         Sunset/Parkside|(37.7487247711275...| NULL|           dummy|\n",
      "|   1030104|   M14|         30612|Medical Incident|04/12/2000|04/12/2000|04/12/2000 09:09:...|04/12/2000 09:10:...|04/12/2000 09:10:...|04/12/2000 09:12:...|04/12/2000 09:19:...|04/12/2000 09:43:...|04/12/2000 10:03:...|               Other|04/12/2000 10:23:...|1700 Block of 43R...|  SF|            94122|      B08|         23|7651|               3|       3|            3|   true|         NULL|             1|   MEDIC|                         2|                     8|                 4|         Sunset/Parkside|(37.7540326780595...| NULL|           dummy|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+------------------------+--------------------+-----+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Number, Unit ID, Incident Number, Call Type, Call Date, Watch Date, Received DtTm, Entry DtTm, Dispatch DtTm, Response DtTm, On Scene DtTm, Transport DtTm, Hospital DtTm, Call Final Disposition, Available DtTm, Address, City, Zipcode of Incident, Battalion, Station Area, Box, Original Priority, Priority, Final Priority, ALS Unit, Call Type Group, Number of Alarms, Unit Type, Unit sequence in call dispatch, Fire Prevention District, Supervisor District, Neighborhooods - Analysis Boundaries, Location, RowID\n",
      " Schema: CallNumber, UnitID, IncidentNumber, CallType, CallDate, WatchDate, ReceivedDtTm, EntryDtTm, DispatchDtTm, ResponseDtTm, OnSceneDtTm, TransportDtTm, HospitalDtTm, CallFinalDisposition, AvailableDtTm, Address, City, ZipcodeofIncident, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumberofAlarms, UnitType, UnitSequenceInCallDispatch, FirePreventionDistrict, SupervisorDistrict, NeighborhooodsAnalysisBoundaries, Location, RowID\n",
      "Expected: CallNumber but found: Call Number\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n"
     ]
    }
   ],
   "source": [
    "# show records with more than 1 day response date\n",
    "added_col.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f63359f-b1e0-41a6-9cce-9f68a5651917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding more than one new cols\n",
    "# Signature:\n",
    "# sf_firecall_df_renamed_col.withColumns(\n",
    "#     *colsMap: Dict[str, pyspark.sql.column.Column],\n",
    "# ) -> 'DataFrame'\n",
    "# Docstring:\n",
    "# Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
    "# existing columns that have the same names.\n",
    "\n",
    "# The colsMap is a map of column name and column, the column must only refer to attributes\n",
    "# supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
    "\n",
    "\n",
    "added_cols = sf_firecall_df_renamed_col.withColumns({\"newcol1\":lit(\"new col data 1\"),\"newcol2\":lit(\"new col data 2\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba29fdd8-1774-44ef-931a-3044d41d34ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+------------------------+--------------------+-----+--------------+--------------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|        ReceivedDtTm|           EntryDtTm|        DispatchDtTm|        ResponseDtTm|         OnSceneDtTm|       TransportDtTm|        HospitalDtTm|CallFinalDisposition|       AvailableDtTm|             Address|City|ZipcodeofIncident|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumberofAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|NeighborhooodsBoundaries|            Location|RowID|       newcol1|       newcol2|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+------------------------+--------------------+-----+--------------+--------------+\n",
      "|   1030101|   E18|        306091|Medical Incident|04/12/2000|04/12/2000|04/12/2000 09:00:...|04/12/2000 09:01:...|04/12/2000 09:02:...|                NULL|                NULL|                NULL|                NULL|               Other|                NULL|2000 Block of 37T...|  SF|            94116|      B08|         18|0757|               3|       3|            3|  false|         NULL|             1|  ENGINE|                         1|                     8|                 4|         Sunset/Parkside|(37.7487247711275...| NULL|new col data 1|new col data 2|\n",
      "|   1030104|   M14|         30612|Medical Incident|04/12/2000|04/12/2000|04/12/2000 09:09:...|04/12/2000 09:10:...|04/12/2000 09:10:...|04/12/2000 09:12:...|04/12/2000 09:19:...|04/12/2000 09:43:...|04/12/2000 10:03:...|               Other|04/12/2000 10:23:...|1700 Block of 43R...|  SF|            94122|      B08|         23|7651|               3|       3|            3|   true|         NULL|             1|   MEDIC|                         2|                     8|                 4|         Sunset/Parkside|(37.7540326780595...| NULL|new col data 1|new col data 2|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+------------------------+--------------------+-----+--------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Number, Unit ID, Incident Number, Call Type, Call Date, Watch Date, Received DtTm, Entry DtTm, Dispatch DtTm, Response DtTm, On Scene DtTm, Transport DtTm, Hospital DtTm, Call Final Disposition, Available DtTm, Address, City, Zipcode of Incident, Battalion, Station Area, Box, Original Priority, Priority, Final Priority, ALS Unit, Call Type Group, Number of Alarms, Unit Type, Unit sequence in call dispatch, Fire Prevention District, Supervisor District, Neighborhooods - Analysis Boundaries, Location, RowID\n",
      " Schema: CallNumber, UnitID, IncidentNumber, CallType, CallDate, WatchDate, ReceivedDtTm, EntryDtTm, DispatchDtTm, ResponseDtTm, OnSceneDtTm, TransportDtTm, HospitalDtTm, CallFinalDisposition, AvailableDtTm, Address, City, ZipcodeofIncident, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumberofAlarms, UnitType, UnitSequenceInCallDispatch, FirePreventionDistrict, SupervisorDistrict, NeighborhooodsAnalysisBoundaries, Location, RowID\n",
      "Expected: CallNumber but found: Call Number\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n"
     ]
    }
   ],
   "source": [
    "added_cols.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d28ccf8-cf25-4e06-bf57-3d63212a8469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+------------------------+--------------------+-----+--------------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|        ReceivedDtTm|           EntryDtTm|        DispatchDtTm|        ResponseDtTm|         OnSceneDtTm|       TransportDtTm|        HospitalDtTm|CallFinalDisposition|       AvailableDtTm|             Address|City|ZipcodeofIncident|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumberofAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|NeighborhooodsBoundaries|            Location|RowID|       newcol2|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+------------------------+--------------------+-----+--------------+\n",
      "|   1030101|   E18|        306091|Medical Incident|04/12/2000|04/12/2000|04/12/2000 09:00:...|04/12/2000 09:01:...|04/12/2000 09:02:...|                NULL|                NULL|                NULL|                NULL|               Other|                NULL|2000 Block of 37T...|  SF|            94116|      B08|         18|0757|               3|       3|            3|  false|         NULL|             1|  ENGINE|                         1|                     8|                 4|         Sunset/Parkside|(37.7487247711275...| NULL|new col data 2|\n",
      "|   1030104|   M14|         30612|Medical Incident|04/12/2000|04/12/2000|04/12/2000 09:09:...|04/12/2000 09:10:...|04/12/2000 09:10:...|04/12/2000 09:12:...|04/12/2000 09:19:...|04/12/2000 09:43:...|04/12/2000 10:03:...|               Other|04/12/2000 10:23:...|1700 Block of 43R...|  SF|            94122|      B08|         23|7651|               3|       3|            3|   true|         NULL|             1|   MEDIC|                         2|                     8|                 4|         Sunset/Parkside|(37.7540326780595...| NULL|new col data 2|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+------------------------+--------------------+-----+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Number, Unit ID, Incident Number, Call Type, Call Date, Watch Date, Received DtTm, Entry DtTm, Dispatch DtTm, Response DtTm, On Scene DtTm, Transport DtTm, Hospital DtTm, Call Final Disposition, Available DtTm, Address, City, Zipcode of Incident, Battalion, Station Area, Box, Original Priority, Priority, Final Priority, ALS Unit, Call Type Group, Number of Alarms, Unit Type, Unit sequence in call dispatch, Fire Prevention District, Supervisor District, Neighborhooods - Analysis Boundaries, Location, RowID\n",
      " Schema: CallNumber, UnitID, IncidentNumber, CallType, CallDate, WatchDate, ReceivedDtTm, EntryDtTm, DispatchDtTm, ResponseDtTm, OnSceneDtTm, TransportDtTm, HospitalDtTm, CallFinalDisposition, AvailableDtTm, Address, City, ZipcodeofIncident, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumberofAlarms, UnitType, UnitSequenceInCallDispatch, FirePreventionDistrict, SupervisorDistrict, NeighborhooodsAnalysisBoundaries, Location, RowID\n",
      "Expected: CallNumber but found: Call Number\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n"
     ]
    }
   ],
   "source": [
    "# dropping cols\n",
    "# Signature: added_cols.drop(*cols: 'ColumnOrName') -> 'DataFrame'\n",
    "# Docstring:\n",
    "# Returns a new :class:`DataFrame` without specified columns.\n",
    "# This is a no-op if the schema doesn't contain the given column name(s).\n",
    "\n",
    "dropped_col_df = added_cols.drop(\"newcol1\")\n",
    "dropped_col_df.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34f65262-a8d5-4f91-a12d-b9a5790ce45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11aa104f-2cec-40fc-9b62-34fdb41d5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find time taken to respond\n",
    "sf_firecall_df_new = sf_firecall_df.withColumns({\"res_date\":unix_timestamp(sf_firecall_df[\"ResponseDtTm\"],'MM/dd/yyyy hh:mm:ss a'), \"rec_date\":unix_timestamp(sf_firecall_df[\"ReceivedDtTm\"],'MM/dd/yyyy hh:mm:ss a')})\n",
    "sf_firecall_df_new = sf_firecall_df_new.withColumn(\"res_time_taken_in_sec\",\n",
    "                                when((col(\"res_date\")-col(\"rec_date\")).isNull(), 0)\\\n",
    "                              .otherwise((col(\"res_date\")-col(\"rec_date\"))))\\\n",
    "        .select(\"IncidentNumber\",\"ResponseDtTm\",\"ReceivedDtTm\",\"res_date\",\"rec_date\",\"res_time_taken_in_sec\")\\\n",
    "\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c99fc9b-c48a-491e-a6a4-e9200c4210e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Incident Number, Received DtTm, Response DtTm\n",
      " Schema: IncidentNumber, ReceivedDtTm, ResponseDtTm\n",
      "Expected: IncidentNumber but found: Incident Number\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "25/03/26 19:38:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Received DtTm\n",
      " Schema: ReceivedDtTm\n",
      "Expected: ReceivedDtTm but found: Received DtTm\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+----------------------+----------+----------+---------------------+\n",
      "|IncidentNumber|ResponseDtTm          |ReceivedDtTm          |res_date  |rec_date  |res_time_taken_in_sec|\n",
      "+--------------+----------------------+----------------------+----------+----------+---------------------+\n",
      "|87458         |10/19/2000 12:45:59 PM|10/18/2000 05:26:22 AM|971939759 |971826982 |112777               |\n",
      "|17066608      |06/08/2017 02:33:46 PM|06/07/2017 07:49:13 AM|1496912626|1496801953|110673               |\n",
      "|9071789       |08/31/2009 03:45:00 PM|08/30/2009 10:45:34 AM|1251713700|1251609334|104366               |\n",
      "|87458         |10/19/2000 09:21:44 AM|10/18/2000 05:26:22 AM|971927504 |971826982 |100522               |\n",
      "|17119230      |10/11/2017 06:55:12 PM|10/10/2017 03:17:46 PM|1507728312|1507628866|99446                |\n",
      "|7101115       |12/19/2007 08:46:56 AM|12/18/2007 06:51:08 AM|1198034216|1197940868|93348                |\n",
      "|1052942       |06/21/2001 10:03:58 AM|06/20/2001 08:11:12 AM|993098038 |993004872 |93166                |\n",
      "|16067026      |06/19/2016 03:48:30 PM|06/18/2016 02:19:13 PM|1466331510|1466239753|91757                |\n",
      "|14023880      |03/13/2014 08:29:54 AM|03/12/2014 07:26:08 AM|1394679594|1394589368|90226                |\n",
      "|9071789       |08/31/2009 11:48:53 AM|08/30/2009 10:45:34 AM|1251699533|1251609334|90199                |\n",
      "|14023880      |03/13/2014 08:28:28 AM|03/12/2014 07:26:08 AM|1394679508|1394589368|90140                |\n",
      "|9071789       |08/31/2009 11:47:42 AM|08/30/2009 10:45:34 AM|1251699462|1251609334|90128                |\n",
      "|14023880      |03/13/2014 08:28:15 AM|03/12/2014 07:26:08 AM|1394679495|1394589368|90127                |\n",
      "|11050532      |06/03/2011 11:42:09 AM|06/02/2011 10:44:21 AM|1307081529|1306991661|89868                |\n",
      "|10093327      |10/21/2010 05:31:12 AM|10/20/2010 05:05:37 AM|1287619272|1287531337|87935                |\n",
      "|9071789       |08/31/2009 10:03:02 AM|08/30/2009 10:45:34 AM|1251693182|1251609334|83848                |\n",
      "|1083981       |10/02/2001 01:07:25 PM|10/01/2001 02:11:10 PM|1002008245|1001925670|82575                |\n",
      "|1083981       |10/02/2001 01:07:05 PM|10/01/2001 02:11:10 PM|1002008225|1001925670|82555                |\n",
      "|1083981       |10/02/2001 01:07:03 PM|10/01/2001 02:11:10 PM|1002008223|1001925670|82553                |\n",
      "|1083981       |10/02/2001 01:06:49 PM|10/01/2001 02:11:10 PM|1002008209|1001925670|82539                |\n",
      "+--------------+----------------------+----------------------+----------+----------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:====================================================>   (13 + 1) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|extracted_year|\n",
      "+--------------+\n",
      "|          2000|\n",
      "|          2001|\n",
      "|          2002|\n",
      "|          2003|\n",
      "|          2004|\n",
      "|          2005|\n",
      "|          2006|\n",
      "|          2007|\n",
      "|          2008|\n",
      "|          2009|\n",
      "|          2010|\n",
      "|          2011|\n",
      "|          2012|\n",
      "|          2013|\n",
      "|          2014|\n",
      "|          2015|\n",
      "|          2016|\n",
      "|          2017|\n",
      "|          2018|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "sf_firecall_df_new.orderBy(col(\"res_time_taken_in_sec\"), ascending=False).show(truncate=False)\n",
    "\n",
    "# extract year from the date\n",
    "from pyspark.sql.functions import year\n",
    "date_ops_df = sf_firecall_df_new.withColumns({\"resdate\":to_timestamp(col(\"ResponseDtTm\"),'MM/dd/yyyy hh:mm:ss a'), \"recdate\":to_timestamp(col(\"ReceivedDtTm\"),'MM/dd/yyyy hh:mm:ss a')})\n",
    "date_ops_df.select(year(\"recdate\").alias(\"extracted_year\")).distinct().orderBy(\"extracted_year\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8675e-7d03-4baa-84b3-933429daa62b",
   "metadata": {},
   "source": [
    "# aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20c0efc3-05fd-4dad-b4c2-aba4603b48c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Type\n",
      " Schema: CallType\n",
      "Expected: CallType but found: Call Type\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "[Stage 19:====================================================>   (13 + 1) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            CallType|  count|\n",
      "+--------------------+-------+\n",
      "|    Medical Incident|3135026|\n",
      "|      Structure Fire| 628420|\n",
      "|              Alarms| 517243|\n",
      "|   Traffic Collision| 197956|\n",
      "|               Other|  77082|\n",
      "|Citizen Assist / ...|  72529|\n",
      "|        Outside Fire|  57213|\n",
      "|        Vehicle Fire|  23178|\n",
      "|        Water Rescue|  22991|\n",
      "|Gas Leak (Natural...|  18393|\n",
      "|   Electrical Hazard|  13580|\n",
      "|Elevator / Escala...|  12728|\n",
      "|Odor (Strange / U...|  12474|\n",
      "|Smoke Investigati...|  10734|\n",
      "|          Fuel Spill|   5593|\n",
      "|              HazMat|   3931|\n",
      "|Industrial Accidents|   2836|\n",
      "|           Explosion|   2587|\n",
      "|  Aircraft Emergency|   1511|\n",
      "|       Assist Police|   1334|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Let’s take our first question: what were the most common types of fire calls?\n",
    "\n",
    "# to check cols on the df sf_firecall_df.columns\n",
    "\n",
    "sf_firecall_df.where(col('CallType').isNotNull())\\\n",
    "                .groupBy('CallType').count()\\\n",
    "                .orderBy('count', ascending = False)\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20eeb9aa-be0f-4a03-8a6e-e6dd88e8050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Along with all the others we’ve seen, the Data‐\n",
    "# Frame API provides descriptive statistical methods like min(), max(), sum(), and\n",
    "# avg(). Let’s take a look at some examples showing how to compute them with our SF\n",
    "# Fire Department data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "367dae96-d937-4a48-b3af-2a0714ccf056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Type, Call Date\n",
      " Schema: CallType, CallDate\n",
      "Expected: CallType but found: Call Type\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "[Stage 22:====================================================>   (13 + 1) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            CallType| count|\n",
      "+--------------------+------+\n",
      "|    Medical Incident|198404|\n",
      "|              Alarms| 32646|\n",
      "|      Structure Fire| 24843|\n",
      "|   Traffic Collision| 12367|\n",
      "|        Outside Fire|  4362|\n",
      "|               Other|  3858|\n",
      "|Citizen Assist / ...|  3842|\n",
      "|Gas Leak (Natural...|  1655|\n",
      "|        Water Rescue|  1341|\n",
      "|        Vehicle Fire|   962|\n",
      "|   Electrical Hazard|   915|\n",
      "|Elevator / Escala...|   867|\n",
      "|Smoke Investigati...|   784|\n",
      "|          Fuel Spill|   259|\n",
      "|Odor (Strange / U...|   208|\n",
      "|Train / Rail Inci...|   144|\n",
      "|              HazMat|   132|\n",
      "|           Explosion|    63|\n",
      "|Industrial Accidents|    51|\n",
      "|Extrication / Ent...|    49|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# What were all the different types of fire calls in 2018?\n",
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# firecalls_2018.select('CallDate','calldate_year').show()\n",
    "\n",
    "firecalls_2018 = sf_firecall_df\\\n",
    "    .where(year(to_date('CallDate', 'mm/dd/yyyy')) == 2018)\\\n",
    "    .withColumn('calldate_year', year(to_date('CallDate', 'mm/dd/yyyy')))\n",
    "\n",
    "firecalls_2018\\\n",
    "    .groupBy('CallType')\\\n",
    "    .count()\\\n",
    "    .orderBy('count', ascending = False)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d26fc3de-067b-4609-a09b-b68a17ec8b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Date\n",
      " Schema: CallDate\n",
      "Expected: CallDate but found: Call Date\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "[Stage 25:====================================================>   (13 + 1) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+-----+\n",
      "|calldate_year|calldate_month|count|\n",
      "+-------------+--------------+-----+\n",
      "|         2018|             1|27027|\n",
      "|         2018|             3|26606|\n",
      "|         2018|            10|26536|\n",
      "|         2018|            11|26307|\n",
      "|         2018|             5|26297|\n",
      "|         2018|             6|26189|\n",
      "|         2018|             7|25964|\n",
      "|         2018|             4|25565|\n",
      "|         2018|             8|25341|\n",
      "|         2018|             9|24602|\n",
      "|         2018|             2|24252|\n",
      "|         2018|            12| 3307|\n",
      "+-------------+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# What months within the year 2018 saw the highest number of fire calls?\n",
    "\n",
    "from pyspark.sql.functions import month\n",
    "firecalls_2018_with_month = firecalls_2018.withColumns({'calldate_year': year(to_date('CallDate', 'MM/dd/yyyy')), 'calldate_month': month(to_date('CallDate', 'MM/dd/yyyy'))})\n",
    "firecalls_2018_with_month\\\n",
    "    .groupBy('calldate_year','calldate_month')\\\n",
    "    .count()\\\n",
    "    .orderBy('count', ascending = False)\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44290654-af9f-43a1-9495-045703506192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Date, Neighborhooods - Analysis Boundaries\n",
      " Schema: CallDate, NeighborhooodsAnalysisBoundaries\n",
      "Expected: CallDate but found: Call Date\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "[Stage 28:====================================================>   (13 + 1) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----+\n",
      "|NeighborhooodsAnalysisBoundaries|count|\n",
      "+--------------------------------+-----+\n",
      "|Tenderloin                      |40537|\n",
      "|South of Market                 |30356|\n",
      "|Mission                         |25210|\n",
      "|Financial District/South Beach  |22228|\n",
      "|Bayview Hunters Point           |14582|\n",
      "|Sunset/Parkside                 |10056|\n",
      "|Western Addition                |9874 |\n",
      "|Nob Hill                        |9091 |\n",
      "|Castro/Upper Market             |7652 |\n",
      "|Hayes Valley                    |7246 |\n",
      "|Outer Richmond                  |6724 |\n",
      "|North Beach                     |6207 |\n",
      "|West of Twin Peaks              |5940 |\n",
      "|Excelsior                       |5708 |\n",
      "|Marina                          |5662 |\n",
      "|Pacific Heights                 |5662 |\n",
      "|Chinatown                       |5586 |\n",
      "|Potrero Hill                    |5086 |\n",
      "|Bernal Heights                  |4605 |\n",
      "|Mission Bay                     |4575 |\n",
      "+--------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Which neighborhood in San Francisco generated the most fire calls in 2018?\n",
    "\n",
    "# firecalls_2018_with_month.columns\n",
    "\n",
    "firecalls_2018_with_month\\\n",
    "    .groupBy('NeighborhooodsAnalysisBoundaries')\\\n",
    "    .count()\\\n",
    "    .orderBy('count', ascending = False)\\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f187c490-7539-4563-b056-bd7729b027d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Call Date\n",
      " Schema: CallDate\n",
      "Expected: CallDate but found: Call Date\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "[Stage 31:====================================================>   (13 + 1) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+------------+\n",
      "|calldate_year|week_of_year|weekly_count|\n",
      "+-------------+------------+------------+\n",
      "|         2018|           1|        6626|\n",
      "|         2018|          25|        6425|\n",
      "|         2018|          22|        6328|\n",
      "|         2018|          13|        6321|\n",
      "|         2018|          27|        6289|\n",
      "|         2018|          40|        6252|\n",
      "|         2018|          44|        6250|\n",
      "|         2018|          16|        6217|\n",
      "|         2018|          46|        6209|\n",
      "|         2018|          43|        6200|\n",
      "|         2018|           5|        6160|\n",
      "|         2018|          18|        6152|\n",
      "|         2018|          48|        6142|\n",
      "|         2018|           2|        6109|\n",
      "|         2018|           9|        6079|\n",
      "|         2018|          21|        6073|\n",
      "|         2018|          45|        6050|\n",
      "|         2018|           6|        6025|\n",
      "|         2018|           8|        6014|\n",
      "|         2018|          23|        5997|\n",
      "+-------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Which week in the year in 2018 had the most fire calls?\n",
    "\n",
    "from pyspark.sql.functions import weekofyear\n",
    "\n",
    "# firecalls_2018.columns\n",
    "firecalls_2018_with_weeks = firecalls_2018.withColumn('week_of_year', weekofyear(to_date('CallDate', 'MM/dd/yyyy')))\n",
    "firecalls_2018_with_weeks.groupBy('calldate_year','week_of_year')\\\n",
    "                            .count().withColumnRenamed('count','weekly_count')\\\n",
    "                            .orderBy('weekly_count', ascending = False)\\\n",
    "                            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46e695a1-ba84-4d1e-895b-51287bfd60e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:38:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Neighborhooods - Analysis Boundaries\n",
      " Schema: NeighborhooodsAnalysisBoundaries\n",
      "Expected: NeighborhooodsAnalysisBoundaries but found: Neighborhooods - Analysis Boundaries\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "25/03/26 19:38:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Zipcode of Incident\n",
      " Schema: ZipcodeofIncident\n",
      "Expected: ZipcodeofIncident but found: Zipcode of Incident\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------+\n",
      "|NeighborhooodsAnalysisBoundaries|total_calls|\n",
      "+--------------------------------+-----------+\n",
      "|Tenderloin                      |634244     |\n",
      "|South of Market                 |460433     |\n",
      "|Mission                         |438958     |\n",
      "|Financial District/South Beach  |329037     |\n",
      "|Bayview Hunters Point           |260228     |\n",
      "|Sunset/Parkside                 |189134     |\n",
      "|Western Addition                |178051     |\n",
      "|Nob Hill                        |159096     |\n",
      "|Outer Richmond                  |129874     |\n",
      "|Hayes Valley                    |118402     |\n",
      "|Castro/Upper Market             |115631     |\n",
      "|West of Twin Peaks              |107252     |\n",
      "|North Beach                     |104359     |\n",
      "|Chinatown                       |102116     |\n",
      "|Pacific Heights                 |99150      |\n",
      "|Excelsior                       |97356      |\n",
      "|Bernal Heights                  |92728      |\n",
      "|Marina                          |90921      |\n",
      "|Potrero Hill                    |88548      |\n",
      "|Inner Sunset                    |77704      |\n",
      "+--------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:====================================================>   (13 + 1) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|ZipcodeofIncident|total_calls|\n",
      "+-----------------+-----------+\n",
      "|94102            |605254     |\n",
      "|94103            |578402     |\n",
      "|94110            |410468     |\n",
      "|94109            |401869     |\n",
      "|94124            |250373     |\n",
      "|94112            |227335     |\n",
      "|94115            |214642     |\n",
      "|94107            |191061     |\n",
      "|94122            |172112     |\n",
      "|94133            |171867     |\n",
      "|94117            |162794     |\n",
      "|94118            |143381     |\n",
      "|94114            |143277     |\n",
      "|94134            |133630     |\n",
      "|94121            |128435     |\n",
      "|94132            |116152     |\n",
      "|94105            |116027     |\n",
      "|94108            |112594     |\n",
      "|94116            |103737     |\n",
      "|94123            |100121     |\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Is there a correlation between neighborhood, zip code, and number of fire calls?\n",
    "\n",
    "# sf_firecall_df.columns\n",
    "neighborhood_total_calls = sf_firecall_df\\\n",
    "                            .groupby('NeighborhooodsAnalysisBoundaries')\\\n",
    "                            .count().withColumnRenamed('count', 'total_calls')\\\n",
    "                            .orderBy('total_calls', ascending = False)\n",
    "neighborhood_total_calls.show(truncate = False)\n",
    "\n",
    "zipcode_total_calls = sf_firecall_df\\\n",
    "                            .groupby('ZipcodeofIncident')\\\n",
    "                            .count().withColumnRenamed('count', 'total_calls')\\\n",
    "                            .orderBy('total_calls', ascending = False)\n",
    "zipcode_total_calls.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bfa458e-ca46-4309-b661-2fc89b2eb1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 19:39:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Zipcode of Incident\n",
      " Schema: ZipcodeofIncident\n",
      "Expected: ZipcodeofIncident but found: Zipcode of Incident\n",
      "CSV file: file:///Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/Fire_Department_Calls_for_Service.csv\n",
      "                                                                                "
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/Users/pvasud669@apac.comcast.com/repos/learnings/spark/zipcode_total_calls already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# How can we use Parquet files or SQL tables to store this data and read it back?\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mzipcode_total_calls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzipcode_total_calls\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/lib/python3.8/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/Users/pvasud669@apac.comcast.com/repos/learnings/spark/zipcode_total_calls already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "# How can we use Parquet files or SQL tables to store this data and read it back?\n",
    "\n",
    "zipcode_total_calls.write.parquet('zipcode_total_calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4173108-cc33-408a-950a-6cab921b1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_parquet_df = spark.read.parquet('zipcode_total_calls')\n",
    "zipcode_parquet_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
