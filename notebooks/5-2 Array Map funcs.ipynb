{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bbac455-7744-4093-b298-22c710081537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95dbaddb-b56e-462d-a942-2513e34b5a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/14 12:35:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# to fix error Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
    "# add below config property .config(\"spark.driver.bindAddress\",\"127.0.0.1\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"array-map-funcs\")\\\n",
    "            .config(\"spark.driver.bindAddress\",\"127.0.0.1\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b2bad0-96fd-46c6-8d60-b929c90cd775",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5eb10a7-a0fd-4ecf-8e63-9c1b27276705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, ArrayType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a6e26d-3479-4cba-b7ea-50f609cd9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"name\",StringType(), True),\n",
    "    StructField(\"lang_at_school\",ArrayType(StringType(),False), True),\n",
    "    StructField(\"lang_at_work\",ArrayType(StringType(),False), True),\n",
    "    StructField(\"current_state\",StringType(), True),\n",
    "    StructField(\"prev_state\",StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7214d025-df87-441d-901d-1ec379b295b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema, verifySchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab611a4-f5cf-4453-972c-10c36a571af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------------+-------------+----------+\n",
      "|            name|    lang_at_school|   lang_at_work|current_state|prev_state|\n",
      "+----------------+------------------+---------------+-------------+----------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|           OH|        CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|           NY|        NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|           UT|        NV|\n",
      "+----------------+------------------+---------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4454bea9-3483-430c-ab7b-b81e3343ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd36a89f-6a72-448e-ada7-b4d0fbe9d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|            name|   col|\n",
      "+----------------+------+\n",
      "|    James,,Smith|  Java|\n",
      "|    James,,Smith| Scala|\n",
      "|    James,,Smith|   C++|\n",
      "|   Michael,Rose,| Spark|\n",
      "|   Michael,Rose,|  Java|\n",
      "|   Michael,Rose,|   C++|\n",
      "|Robert,,Williams|CSharp|\n",
      "|Robert,,Williams|    VB|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode()\n",
    "# Use explode() function to create a new row for each element in the given array column. \n",
    "# There are various PySpark SQL explode functions available to work with Array columns.\n",
    "\n",
    "\n",
    "df.select(df.name, f.explode(df.lang_at_school)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f289f7b-c8d5-4cb1-b6ac-d4d849055a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|actual_name_as_str|      array_of_names|\n",
      "+------------------+--------------------+\n",
      "|      James,,Smith|    [James, , Smith]|\n",
      "|     Michael,Rose,|   [Michael, Rose, ]|\n",
      "|  Robert,,Williams|[Robert, , Williams]|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split()\n",
    "\n",
    "# split() sql function returns an array type after splitting the string column by delimiter. \n",
    "# Below example split the name column by comma delimiter.\n",
    "\n",
    "df.select(df.name.alias(\"actual_name_as_str\"),f.split(df.name, \",\").alias(\"array_of_names\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb5edc83-b5de-4712-99ea-63cca2a429bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|            name|states_lived|\n",
      "+----------------+------------+\n",
      "|    James,,Smith|    [OH, CA]|\n",
      "|   Michael,Rose,|    [NY, NJ]|\n",
      "|Robert,,Williams|    [UT, NV]|\n",
      "+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array()\n",
    "\n",
    "# Use array() function to create a new array column by merging the data from multiple columns. All input columns must have the same data type. \n",
    "# The below example combines the data from currentState and previousState and creates a new column states.\n",
    "\n",
    "df.select(df.name,f.array(df.current_state, df.prev_state).alias(\"states_lived\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "367751eb-91e5-4d34-9b71-d5051824b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|array_contains(lang_at_school, Java)|\n",
      "+------------------------------------+\n",
      "|                                true|\n",
      "|                                true|\n",
      "|                               false|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_contains()\n",
    "\n",
    "# array_contains() sql function is used to check if array column contains a value. \n",
    "# Returns null if the array is null, true if the array contains the value, and false otherwise.\n",
    "\n",
    "df.select(f.array_contains(df.lang_at_school, \"Java\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d83aa3f9-70c3-4f03-ba58-9351f636f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"all_langs\", f.concat(df.lang_at_school, df.lang_at_work))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af8cccff-239c-4669-8a55-568c4140350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------------+-------------+----------+--------------------+\n",
      "|            name|    lang_at_school|   lang_at_work|current_state|prev_state|           all_langs|\n",
      "+----------------+------------------+---------------+-------------+----------+--------------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|           OH|        CA|[Java, Scala, C++...|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|           NY|        NJ|[Spark, Java, C++...|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|           UT|        NV|[CSharp, VB, Spar...|\n",
      "+----------------+------------------+---------------+-------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a525a625-93ee-431c-b63e-ea46ce2a1cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+---------------------------+\n",
      "|all_langs                      |distinct_langs             |\n",
      "+-------------------------------+---------------------------+\n",
      "|[Java, Scala, C++, Spark, Java]|[Java, Scala, C++, Spark]  |\n",
      "|[Spark, Java, C++, Spark, Java]|[Spark, Java, C++]         |\n",
      "|[CSharp, VB, Spark, Python]    |[CSharp, VB, Spark, Python]|\n",
      "+-------------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removes duplicates within an array\n",
    "\n",
    "df.select(df.all_langs, f.array_distinct(df.all_langs).alias(\"distinct_langs\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7190806c-1331-4994-a8eb-e9f0e1dadb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------+\n",
      "|name            |langs_at_school_and_work|\n",
      "+----------------+------------------------+\n",
      "|James,,Smith    |[Java]                  |\n",
      "|Michael,Rose,   |[Spark, Java]           |\n",
      "|Robert,,Williams|[]                      |\n",
      "+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns the intersection of two arrays without duplicates\n",
    "\n",
    "df.select(df.name, f.array_intersect(df.lang_at_school, df.lang_at_work).alias(\"langs_at_school_and_work\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84d3b098-3f1a-4e8d-a339-8ffb47264d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------------+\n",
      "|name            |all_langs_unique           |\n",
      "+----------------+---------------------------+\n",
      "|James,,Smith    |[Java, Scala, C++, Spark]  |\n",
      "|Michael,Rose,   |[Spark, Java, C++]         |\n",
      "|Robert,,Williams|[CSharp, VB, Spark, Python]|\n",
      "+----------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns the union of two arrays without duplicates\n",
    "\n",
    "df.select(df.name, f.array_union(df.lang_at_school, df.lang_at_work).alias(\"all_langs_unique\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce4239c8-5e29-4d4a-8a9e-359c1bac3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------------------+\n",
      "|name            |langs_at_school_and_not_in_work|\n",
      "+----------------+-------------------------------+\n",
      "|James,,Smith    |[Scala, C++]                   |\n",
      "|Michael,Rose,   |[C++]                          |\n",
      "|Robert,,Williams|[CSharp, VB]                   |\n",
      "+----------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns elements in array1 but not in array2, without duplicates\n",
    "\n",
    "df.select(df.name, f.array_except(df.lang_at_school, df.lang_at_work).alias(\"langs_at_school_and_not_in_work\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d778fd4-88a3-412a-be04-900bbd7dba09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|name            |lang_at_school  |\n",
      "+----------------+----------------+\n",
      "|James,,Smith    |Java--Scala--C++|\n",
      "|Michael,Rose,   |Spark--Java--C++|\n",
      "|Robert,,Williams|CSharp--VB      |\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenates the elements of an array using a delimiter\n",
    "\n",
    "df.select(df.name, f.array_join(df.lang_at_school, \"--\").alias(\"lang_at_school\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69cf363a-a185-48c3-969a-8b6fd1168b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------------------+\n",
      "|name            |lang_at_school    |lang_at_school_array_max|\n",
      "+----------------+------------------+------------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Scala                   |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Spark                   |\n",
      "|Robert,,Williams|[CSharp, VB]      |VB                      |\n",
      "+----------------+------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns the maximum value within the array; null elements are skipped\n",
    "\n",
    "df.select(df.name, df.lang_at_school, f.array_max(df.lang_at_school).alias(\"lang_at_school_array_max\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "849dbb8b-d2d4-4027-9e04-ef7fc91f2dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------------------+\n",
      "|name            |lang_at_school    |lang_at_school_array_min|\n",
      "+----------------+------------------+------------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|C++                     |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|C++                     |\n",
      "|Robert,,Williams|[CSharp, VB]      |CSharp                  |\n",
      "+----------------+------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns the minimum value within the array; null elements are skipped\n",
    "\n",
    "df.select(df.name, df.lang_at_school, f.array_min(df.lang_at_school).alias(\"lang_at_school_array_min\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb0ef7c4-dedb-431c-870a-c189ddb889e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the (1-based) index of the first element of the given array as a Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff8dada7-dd19-4605-b749-938db308462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------------------+\n",
      "|    lang_at_school|array_position(lang_at_school, Java)|\n",
      "+------------------+------------------------------------+\n",
      "|[Java, Scala, C++]|                                   1|\n",
      "|[Spark, Java, C++]|                                   2|\n",
      "|      [CSharp, VB]|                                   0|\n",
      "+------------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.lang_at_school, f.array_position(df.lang_at_school, \"Java\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15fdbce2-9e43-46d2-bde4-680f7535a0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----------------------------+\n",
      "|all_langs                      |array_remove(all_langs, Java)|\n",
      "+-------------------------------+-----------------------------+\n",
      "|[Java, Scala, C++, Spark, Java]|[Scala, C++, Spark]          |\n",
      "|[Spark, Java, C++, Spark, Java]|[Spark, C++, Spark]          |\n",
      "|[CSharp, VB, Spark, Python]    |[CSharp, VB, Spark, Python]  |\n",
      "+-------------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removes all elements that are equal to the given element from the given array\n",
    "\n",
    "df.select(df.all_langs, f.array_remove(df.all_langs, \"Java\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6217ae67-1126-47f9-a680-a69b5bacc0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+------------+\n",
      "|    lang_at_school|   lang_at_work|lang_overlap|\n",
      "+------------------+---------------+------------+\n",
      "|[Java, Scala, C++]|  [Spark, Java]|        true|\n",
      "|[Spark, Java, C++]|  [Spark, Java]|        true|\n",
      "|      [CSharp, VB]|[Spark, Python]|       false|\n",
      "+------------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eturns true if array1 contains at least one non-null element also present in array2\n",
    "\n",
    "df.select(df.lang_at_school, df.lang_at_work,f.arrays_overlap(df.lang_at_school, df.lang_at_work).alias(\"lang_overlap\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28e26109-ee0e-4801-bc1f-a9b371b82a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|    lang_at_school|      sorted_langs|\n",
      "+------------------+------------------+\n",
      "|[Java, Scala, C++]|[C++, Java, Scala]|\n",
      "|[Spark, Java, C++]|[C++, Java, Spark]|\n",
      "|      [CSharp, VB]|      [CSharp, VB]|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorts the input array in ascending order, with null elements placed at the end of the array\n",
    "\n",
    "df.select(df.lang_at_school, f.array_sort(df.lang_at_school).alias(\"sorted_langs\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd50a567-8531-46e5-bed2-679156f5d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+------------------------------------+\n",
      "|lang_at_school    |lang_at_work   |concat(lang_at_school, lang_at_work)|\n",
      "+------------------+---------------+------------------------------------+\n",
      "|[Java, Scala, C++]|[Spark, Java]  |[Java, Scala, C++, Spark, Java]     |\n",
      "|[Spark, Java, C++]|[Spark, Java]  |[Spark, Java, C++, Spark, Java]     |\n",
      "|[CSharp, VB]      |[Spark, Python]|[CSharp, VB, Spark, Python]         |\n",
      "+------------------+---------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenates strings, binaries, arrays, etc.\n",
    "\n",
    "df.select(df.lang_at_school, df.lang_at_work, f.concat(df.lang_at_school, df.lang_at_work)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ca71bb0-2922-4ef5-8b72-f44fb8203b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|array_of_langs_array               |\n",
      "+-----------------------------------+\n",
      "|[[Java, Scala, C++], [Spark, Java]]|\n",
      "|[[Spark, Java, C++], [Spark, Java]]|\n",
      "|[[CSharp, VB], [Spark, Python]]    |\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating array of arrays to implement flatten func\n",
    "\n",
    "df = df.withColumn(\"array_of_langs_array\", f.array(df.lang_at_school, df.lang_at_work))\n",
    "df.select(df.array_of_langs_array).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62b63fe8-5293-4c09-9483-5a31f00e76a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- lang_at_school: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- lang_at_work: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- current_state: string (nullable = true)\n",
      " |-- prev_state: string (nullable = true)\n",
      " |-- all_langs: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- array_of_langs_array: array (nullable = false)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b8a6ded-1f08-4fe1-a7cb-26ee743b67c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------------------------+\n",
      "|array_of_langs_array               |flatten(array_of_langs_array)  |\n",
      "+-----------------------------------+-------------------------------+\n",
      "|[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++, Spark, Java]|\n",
      "|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++, Spark, Java]|\n",
      "|[[CSharp, VB], [Spark, Python]]    |[CSharp, VB, Spark, Python]    |\n",
      "+-----------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flattens an array of arrays into a single array\n",
    "\n",
    "df.select(df.array_of_langs_array, f.flatten(df.array_of_langs_array)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2da58b40-cd78-43a4-a5c1-2150373c3030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|repeat_vals          |\n",
      "+---------------------+\n",
      "|[spark, spark, spark]|\n",
      "|[spark, spark, spark]|\n",
      "|[spark, spark, spark]|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns an array containing the specified element the specified number of times\n",
    "\n",
    "# df.withColumn(\"repeat_vals\",f.array_repeat(\"spark\", 3))\n",
    "\n",
    "# above line of code throws error\n",
    "# explnation:\n",
    "# In PySpark, when using array_repeat within a withColumn method, the first argument of array_repeat must be a column, not a literal string.\n",
    "# To fix this, you can use F.lit() to convert your string \"spark\" into a literal\n",
    "\n",
    "df.withColumn(\"repeat_vals\",f.array_repeat(f.lit(\"spark\"), 3)).select(\"repeat_vals\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1810abc1-bdb8-4cac-8b3a-df869e2b3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+\n",
      "|all_langs                      |reverse(all_langs)             |\n",
      "+-------------------------------+-------------------------------+\n",
      "|[Java, Scala, C++, Spark, Java]|[Java, Spark, C++, Scala, Java]|\n",
      "|[Spark, Java, C++, Spark, Java]|[Java, Spark, C++, Java, Spark]|\n",
      "|[CSharp, VB, Spark, Python]    |[Python, Spark, VB, CSharp]    |\n",
      "+-------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns a reversed string or an array with the reverse order of elements\n",
    "\n",
    "df.select(df.all_langs, f.reverse(df.all_langs)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc9635bc-fd00-4478-9cca-09869d5e45b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|sequence(1, 10, 2)|\n",
      "+------------------+\n",
      "|   [1, 3, 5, 7, 9]|\n",
      "|   [1, 3, 5, 7, 9]|\n",
      "|   [1, 3, 5, 7, 9]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generates an array of elements from start to stop (inclusive) by incremental step\n",
    "\n",
    "df.select(f.sequence(f.lit(1),f.lit(10),f.lit(2))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af6579ef-9d1d-4f6c-9310-2f5f0137454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a random permutation of the given array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8352c46-8858-4f61-a7ac-0354a8deb826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+\n",
      "|all_langs                      |shuffle(all_langs)             |\n",
      "+-------------------------------+-------------------------------+\n",
      "|[Java, Scala, C++, Spark, Java]|[Java, Scala, Java, Spark, C++]|\n",
      "|[Spark, Java, C++, Spark, Java]|[Spark, Spark, C++, Java, Java]|\n",
      "|[CSharp, VB, Spark, Python]    |[VB, Spark, Python, CSharp]    |\n",
      "+-------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.all_langs, f.shuffle(df.all_langs)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16d02e4f-0526-4017-91be-fcf5e71eff4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------+\n",
      "|all_langs                      |slice(all_langs, 2, 5)   |\n",
      "+-------------------------------+-------------------------+\n",
      "|[Java, Scala, C++, Spark, Java]|[Scala, C++, Spark, Java]|\n",
      "|[Spark, Java, C++, Spark, Java]|[Java, C++, Spark, Java] |\n",
      "|[CSharp, VB, Spark, Python]    |[VB, Spark, Python]      |\n",
      "+-------------------------------+-------------------------+\n",
      "\n",
      "+-------------------------------+----------------------+\n",
      "|all_langs                      |slice(all_langs, 2, 2)|\n",
      "+-------------------------------+----------------------+\n",
      "|[Java, Scala, C++, Spark, Java]|[Scala, C++]          |\n",
      "|[Spark, Java, C++, Spark, Java]|[Java, C++]           |\n",
      "|[CSharp, VB, Spark, Python]    |[VB, Spark]           |\n",
      "+-------------------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns a subset of the given array starting from the given index (counting from the end if the index is negative), of the specified length\n",
    "# Signature:\n",
    "# f.slice(\n",
    "#     x: 'ColumnOrName',\n",
    "#     start: Union[ForwardRef('ColumnOrName'), int],\n",
    "#     length: Union[ForwardRef('ColumnOrName'), int],\n",
    "# ) -> pyspark.sql.column.Column\n",
    "\n",
    "\n",
    "df.select(df.all_langs, f.slice(df.all_langs, 2, 5)).show(truncate=False)\n",
    "df.select(df.all_langs, f.slice(df.all_langs, 2, 2)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1981bd-0338-4d14-8f3c-885545c0473f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0308001f-64c2-420b-95cd-ee6a36e68e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|arrays_zip(array(1, 2), array(2, 3), array(3, 4))|\n",
      "+-------------------------------------------------+\n",
      "|[{1, 2, 3}, {2, 3, 4}]                           |\n",
      "+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# returns a merged array of structs\n",
    "\n",
    "spark.sql(\"SELECT arrays_zip(array(1, 2),array(2, 3), array(3, 4))\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eea43d69-53ae-4944-8ade-a486dec68e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the element of the given array at the given (1-based) index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94c96802-d554-4f0d-8ded-75b1481b6688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------------+\n",
      "|    lang_at_school|element_at(lang_at_school, -1)|\n",
      "+------------------+------------------------------+\n",
      "|[Java, Scala, C++]|                           C++|\n",
      "|[Spark, Java, C++]|                           C++|\n",
      "|      [CSharp, VB]|                            VB|\n",
      "+------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.lang_at_school, f.element_at(df.lang_at_school, -1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1139df3f-f0d1-472c-a90c-6b9434dc86dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------+\n",
      "|all_langs                      |num_of_unique_langs|\n",
      "+-------------------------------+-------------------+\n",
      "|[Java, Scala, C++, Spark, Java]|4                  |\n",
      "|[Spark, Java, C++, Spark, Java]|3                  |\n",
      "|[CSharp, VB, Spark, Python]    |4                  |\n",
      "+-------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An alias of size; returns the size of the given array or a map\n",
    "\n",
    "df.select(df.all_langs, f.cardinality(f.array_distinct(df.all_langs)).alias(\"num_of_unique_langs\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9da92fd6-1db5-49f7-bd47-194002a31eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_form_arrays(array<K>,array<V>): map<K, V> Creates a map from the given pair of key/value arrays; elements in keys should not be null\n",
    "\n",
    "data2 = [\n",
    "    [[\"name\", \"state\"], [\"peter\", \"NY\"]],\n",
    "    [[\"name\", \"job\"], [\"peter\", \"fireman\"]]\n",
    "]\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"keys\",ArrayType(StringType(), True)),\n",
    "    StructField(\"values\",ArrayType(StringType(), True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5163c6d0-81bd-4dbc-b00c-fdb5cb1f7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(data2, schema2, verifySchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc713ecc-bec0-47e8-ba24-4eb1eccba708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|         keys|          values|\n",
      "+-------------+----------------+\n",
      "|[name, state]|     [peter, NY]|\n",
      "|  [name, job]|[peter, fireman]|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2837e6ed-3795-46ca-9f44-d87a96a629bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"mapped_data\", f.map_from_arrays(df2.keys, df2.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "faade7cd-e058-4a63-8633-06c349b3100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-------------------------------+\n",
      "|keys         |values          |mapped_data                    |\n",
      "+-------------+----------------+-------------------------------+\n",
      "|[name, state]|[peter, NY]     |{name -> peter, state -> NY}   |\n",
      "|[name, job]  |[peter, fireman]|{name -> peter, job -> fireman}|\n",
      "+-------------+----------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd52eb3-330e-425e-b296-2220f018abad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7e9240a-5f5f-46e6-a04a-13327eeabe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f901d736-feec-41cc-b23b-9d4d2543a5ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "dataType <Row(StringType())> should be an instance of <class 'pyspark.sql.types.DataType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# this is not solved, having issue implementing this func\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# map_from_entries(array<struct<K, V>>): map<K, V>   Returns a map created from the given array\u001b[39;00m\n\u001b[1;32m      5\u001b[0m entries \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeter\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNY\u001b[39m\u001b[38;5;124m\"\u001b[39m),]\n\u001b[1;32m      7\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mStructField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStringType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m ])\n\u001b[1;32m     10\u001b[0m df3 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(entries, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     11\u001b[0m df3\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/spark/lib/python3.8/site-packages/pyspark/sql/types.py:680\u001b[0m, in \u001b[0;36mStructField.__init__\u001b[0;34m(self, name, dataType, nullable, metadata)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    675\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    678\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    679\u001b[0m ):\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataType, DataType), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataType \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m should be an instance of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    681\u001b[0m         dataType,\n\u001b[1;32m    682\u001b[0m         DataType,\n\u001b[1;32m    683\u001b[0m     )\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m should be a string\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (name)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n",
      "\u001b[0;31mAssertionError\u001b[0m: dataType <Row(StringType())> should be an instance of <class 'pyspark.sql.types.DataType'>"
     ]
    }
   ],
   "source": [
    "# # this is not solved, having issue implementing this func\n",
    "\n",
    "# # map_from_entries(array<struct<K, V>>): map<K, V>   Returns a map created from the given array\n",
    "\n",
    "# entries = [(\"name\", \"peter\"), (\"state\", \"NY\"),]\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"data\", t.Row(t.StringType()))\n",
    "# ])\n",
    "# df3 = spark.createDataFrame(entries, [\"data\"])\n",
    "# df3.show()\n",
    "\n",
    "# # # spark.createDataFrame(f.map_from_entries(df3._1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5f997-f363-4e91-8205-65729cd0773f",
   "metadata": {},
   "source": [
    "## Higher-Order Functions\n",
    "In addition to the previously noted built-in functions, there are higher-order functions\n",
    "that take anonymous lambda functions as arguments. An example of a higherorder\n",
    "function is the following:\n",
    "\n",
    "-- In SQL\n",
    "### `transform(values, value -> lambda expression)`\n",
    "\n",
    "The transform() function takes an array (values) and anonymous function (lambda\n",
    "expression) as input. The function transparently creates a new array by applying the\n",
    "anonymous function to each element, and then assigning the result to the output\n",
    "array (similar to the UDF approach, but more efficiently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "134396a4-6852-4169-97f8-e0d21d1de216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s create a sample data set so we can run some examples:\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"celsius\", ArrayType(IntegerType()))\n",
    "])\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "\n",
    "df4 = spark.createDataFrame(t_list, schema)\n",
    "df4.createOrReplaceTempView(\"celsius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48dc2b20-db6f-4618-8cf6-231b427e7625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+\n",
      "|celsius                     |reduced_temp                |\n",
      "+----------------------------+----------------------------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|[30, 31, 27, 25, 35, 37, 33]|\n",
      "|[31, 32, 34, 55, 56]        |[26, 27, 29, 50, 51]        |\n",
      "+----------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select celsius, transform(celsius, t -> t-5) as reduced_temp from celsius\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aefa54-4cad-4d7a-85d5-78dde2d39293",
   "metadata": {},
   "source": [
    "### `filter(array<T>, function<T, Boolean>): array<T>`\n",
    "The filter() function produces an array consisting of only the elements of the input\n",
    "array for which the Boolean function is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "43edc1f0-12a3-4137-838b-f3d910df212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|filtered|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|    [42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter temp more than 40 c\n",
    "\n",
    "spark.sql(\"select celsius, filter(celsius, t -> t > 40) as filtered from celsius\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0f48f-718a-4d13-ba49-232aa24e6d23",
   "metadata": {},
   "source": [
    "### `exists(array<T>, function<T, V, Boolean>): Boolean`\n",
    "The exists() function returns true if the Boolean function holds for any element in\n",
    "the input array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "92d2dbba-6fd8-4857-8f9e-75fea8b3ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|             celsius|exists|\n",
      "+--------------------+------+\n",
      "|[35, 36, 32, 30, ...|  true|\n",
      "|[31, 32, 34, 55, 56]| false|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# // Is there a temperature of 38C in the array of temperatures\n",
    "\n",
    "spark.sql(\"select celsius, exists(celsius, t -> t = 38) as exists from celsius\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e689b67-646d-49de-8cfa-63f1e78db66a",
   "metadata": {},
   "source": [
    "### `reduce(array<T>, B, function<B, T, B>, function<B, R>)`\n",
    "The reduce() function reduces the elements of the array to a single value by merging\n",
    "the elements into a buffer B using function<B, T, B> and applying a finishing\n",
    "function<B, R> on the final buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c151b61-6204-4139-a078-25ba158dcfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fba151-8162-4e5d-9f23-403f3b47ed4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27651584-126d-4c8c-a47c-65ef0fc15e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
