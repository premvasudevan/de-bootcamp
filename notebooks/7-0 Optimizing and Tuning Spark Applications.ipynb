{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecaa1bc-a42a-41bd-b3a9-8d7ce1f29de7",
   "metadata": {},
   "source": [
    "## Viewing and Setting Apache Spark Configurations\n",
    "\n",
    "There are three ways you can get and set Spark properties.\n",
    "\n",
    "1. The first is through a set of\n",
    "configuration files. In your deployment’s $SPARK_HOME directory (where you installed\n",
    "Spark), there are a number of config files: conf/spark-defaults.conf.template, conf/\n",
    "log4j.properties.template, and conf/spark-env.sh.template. Changing the default values\n",
    "in these files and saving them without the .template suffix instructs Spark to use these\n",
    "new values\n",
    "\n",
    "2. The second way is to specify Spark configurations directly in your Spark application\n",
    "or on the command line when submitting the application with spark-submit, using\n",
    "the --conf flag:\n",
    "```spark-submit --conf spark.sql.shuffle.partitions=5 --conf\n",
    "\"spark.executor.memory=2g\" --class main.scala.chapter7.SparkConfig_7_1 jars/mainscala-\n",
    "chapter7_2.12-1.0.jar\n",
    "```\n",
    "\n",
    "Here’s how you would do this in the Spark application itself:\n",
    "\n",
    "```def main(args: Array[String]) {\n",
    "// Create a session\n",
    "val spark = SparkSession.builder\n",
    ".config(\"spark.sql.shuffle.partitions\", 5)\n",
    ".config(\"spark.executor.memory\", \"2g\")\n",
    ".master(\"local[*]\")\n",
    ".appName(\"SparkConfig\")\n",
    ".getOrCreate()\n",
    "printConfigs(spark)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\n",
    "spark.sparkContext.defaultParallelism)\n",
    "println(\" ****** Setting Shuffle Partitions to Default Parallelism\")\n",
    "printConfigs(spark)\n",
    "```\n",
    "\n",
    "3. The third option is through a programmatic interface via the Spark shell. As with\n",
    "everything else in Spark, APIs are the primary method of interaction. Through the\n",
    "SparkSession object, you can access most Spark config settings.\n",
    "\n",
    "You can also view only the Spark SQL–specific Spark configs:\n",
    "\n",
    "// In Scala\n",
    "`spark.sql(\"SET -v\").select(\"key\", \"value\").show(5, false)`\n",
    " In Python\n",
    "`spark.sql(\"SET -v\").select(\"key\", \"value\").show(n=5, truncate=False)`\n",
    "\n",
    "|key|value |\n",
    "|----|----|\n",
    "|spark.sql.adaptive.enabled |false |\n",
    "|spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin |0.2 |\n",
    "|spark.sql.adaptive.shuffle.fetchShuffleBlocksInBatch.enabled|true |\n",
    "|spark.sql.adaptive.shuffle.localShuffleReader.enabled |true |\n",
    "|spark.sql.adaptive.shuffle.maxNumPostShufflePartitions |<\"undefined\">|\n",
    "\n",
    "\n",
    "Alternatively, you can access Spark’s current configuration through the Spark UI’s\n",
    "Environment tab, which we discuss later in this chapter, as read-only values\n",
    "\n",
    "To set or modify an existing configuration programmatically, first check if the property\n",
    "is modifiable. `spark.conf.isModifiable(\"<config_name>\")` will return `true` or\n",
    "`false`. All modifiable configs can be set to new values using the API:\n",
    "In Python\n",
    "```\n",
    ">>> spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "'200'\n",
    ">>> spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    ">>> spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "'5'\n",
    "```\n",
    "\n",
    "## Spark properties, an order of precedence\n",
    "Among all the ways that you can set Spark properties, an order of precedence determines\n",
    "which values are honored. Any values or flags defined in spark-defaults.conf\n",
    "will be read first, followed by those supplied on the command line with sparksubmit,\n",
    "and finally those set via SparkSession in the Spark application. \n",
    "\n",
    "All these properties will be merged, with any duplicate properties reset in the Spark application\n",
    "taking precedence. Likewise, values supplied on the command line will supersede settings\n",
    "in the configuration file, provided they are not overwritten in the application itself.\n",
    "\n",
    "\n",
    "## Scaling Spark for Large Workloads\n",
    "there are a handful of Spark configurations that\n",
    "you can enable or alter. These configurations affect three Spark components: the\n",
    "**Spark driver, the executor, and the shuffle service** running on the executor.\n",
    "\n",
    "### Static versus dynamic resource allocation\n",
    "When you specify compute resources as command-line arguments to spark-submit,\n",
    "as we did earlier, you cap the limit. This means that if more resources are needed later\n",
    "as tasks queue up in the driver due to a larger than anticipated workload, Spark cannot\n",
    "accommodate or allocate extra resources.\n",
    "\n",
    "If instead you use Spark’s dynamic resource allocation configuration, the Spark driver\n",
    "can request more or fewer compute resources as the demand of large workloads flows\n",
    "and ebbs.\n",
    "\n",
    "One use case where this can be helpful is streaming, where the data flow volume may\n",
    "be uneven. Another is on-demand data analytics, where you might have a high volume\n",
    "of SQL queries during peak hours. Enabling dynamic resource allocation allows\n",
    "Spark to achieve better utilization of resources, freeing executors when not in use and\n",
    "acquiring new ones when needed\n",
    "\n",
    "To enable and configure dynamic allocation, you can use settings like the following.\n",
    "Note that the numbers here are arbitrary; the appropriate settings will depend on the\n",
    "nature of your workload and they should be adjusted accordingly. Some of these\n",
    "configs cannot be set inside a Spark REPL, so you will have to set them\n",
    "programmatically:\n",
    "\n",
    "``` spark.dynamicAllocation.enabled true\n",
    "spark.dynamicAllocation.minExecutors 2\n",
    "spark.dynamicAllocation.schedulerBacklogTimeout 1m\n",
    "spark.dynamicAllocation.maxExecutors 20\n",
    "spark.dynamicAllocation.executorIdleTimeout 2min\n",
    "```\n",
    "\n",
    "By default spark.dynamicAllocation.enabled is set to false. When enabled with\n",
    "the settings shown here, the Spark driver will request that the cluster manager create\n",
    "two executors to start with, as a minimum (spark.dynamicAllocation.minExecu\n",
    "tors). As the task queue backlog increases, new executors will be requested each time\n",
    "the backlog timeout (spark.dynamicAllocation.schedulerBacklogTimeout) is\n",
    "exceeded. In this case, whenever there are pending tasks that have not been scheduled\n",
    "for over 1 minute, the driver will request that a new executor be launched to schedule\n",
    "backlogged tasks, up to a maximum of 20 (spark.dynamicAllocation.maxExecu\n",
    "tors). By contrast, if an executor finishes a task and is idle for 2 minutes\n",
    "(spark.dynamicAllocation.executorIdleTimeout), the Spark driver will terminate\n",
    "it.\n",
    "\n",
    "### Configuring Spark executors’ memory and the shuffle service \n",
    "\n",
    "Simply enabling dynamic resource allocation is not sufficient. You also have to understand\n",
    "how executor memory is laid out and used by Spark so that executors are not\n",
    "starved of memory or troubled by JVM garbage collection.\n",
    "\n",
    "The amount of memory available to each executor is controlled by\n",
    "`spark.executor.memory`. This is divided into three sections, as depicted in\n",
    "Figure 7-2: execution memory, storage memory, and reserved memory. The default\n",
    "division is 60% for execution memory and 40% for storage, after allowing for 300 MB\n",
    "for reserved memory, to safeguard against OOM errors. The Spark documentation\n",
    "advises that this will work for most cases, but you can adjust what fraction of\n",
    "spark.executor.memory you want either section to use as a baseline. When storage\n",
    "memory is not being used, Spark can acquire it for use in execution memory for execution\n",
    "purposes, and vice versa.\n",
    "\n",
    "Execution memory is used for Spark shuffles, joins, sorts, and aggregations. Since different\n",
    "queries may require different amounts of memory, the fraction (**spark.mem\n",
    "ory.fraction is 0.6 by default**) of the available memory to dedicate to this can be\n",
    "tricky to tune but it’s easy to adjust. By contrast, storage memory is primarily used for\n",
    "caching user data structures and partitions derived from DataFrames.\n",
    "\n",
    "During map and shuffle operations, Spark writes to and reads from the local disk’s\n",
    "shuffle files, so there is heavy I/O activity. This can result in a bottleneck, because the\n",
    "default configurations are suboptimal for large-scale Spark jobs. Knowing what configurations\n",
    "to tweak can mitigate this risk during this phase of a Spark job.\n",
    "\n",
    "In Table 7-1, we capture a few recommended configurations to adjust so that the map,\n",
    "spill, and merge processes during these operations are not encumbered by inefficient\n",
    "I/O and to enable these operations to employ buffer memory before writing the final\n",
    "shuffle partitions to disk. Tuning the shuffle service running on each executor can\n",
    "also aid in increasing overall performance for large Spark workloads\n",
    "\n",
    "**Table 7-1,**\n",
    "|setting|desc|\n",
    "|-----|------|\n",
    "|`spark.driver.memory` | Default is 1g (1 GB). This is the amount of memory allocated to the Spark driver to receive data from executors. This is often changed during sparksubmit with --driver-memory. Only change this if you expect the driver to receive large amounts of data back from operations like collect(), or if you run out of driver memory.|\n",
    "|`spark.shuffle.file.buffer`| Default is 32 KB. Recommended is 1 MB. This allows Spark to do more buffering before writing final map results to disk.|\n",
    "|`spark.file.transferTo` |Default is true. Setting it to false will force Spark to use the file buffer to transfer files before finally writing to disk; this will decrease the I/O activity.|\n",
    "|`spark.shuffle.unsafe.file.output.buffer` |Default is 32 KB. This controls the amount of buffering possible when merging files during shuffle operations. In general, large values (e.g., 1 MB) are more appropriate for larger workloads, whereas the default can work for smaller workloads.|\n",
    "|`spark.io.compression.lz4.blockSize` | Default is 32 KB. Increase to 512 KB. You can decrease the size of the shuffle file by increasing the compressed size of the block.|\n",
    "|`spark.shuffle.service.index.cache.size` | Default is 100m. Cache entries are limited to the specified memory footprintin byte.|\n",
    "|`spark.shuffle.registration.timeout` | Default is 5000 ms. Increase to 120000 ms.|\n",
    "|`spark.shuffle.registration.maxAttempts`| Default is 3. Increase to 5 if needed.|\n",
    "\n",
    "### Maximizing Spark parallelism\n",
    "Spark will at best schedule a thread per task per\n",
    "core, and each task will process a distinct partition. To optimize resource utilization\n",
    "and maximize parallelism, **the ideal is at least as many partitions as there are cores on\n",
    "the executor**\n",
    "\n",
    "### How partitions are created. \n",
    "As mentioned previously, Spark’s tasks process data as partitions\n",
    "read from disk into memory. Data on disk is laid out in chunks or contiguous\n",
    "file blocks, depending on the store. By default, file blocks on data stores range in size\n",
    "from 64 MB to 128 MB. For example, on HDFS and S3 the default size is 128 MB\n",
    "(this is configurable). A contiguous collection of these blocks constitutes a partition.\n",
    "The size of a partition in Spark is dictated by `spark.sql.files.maxPartitionBytes`.\n",
    "The default is 128 MB. You can decrease the size, but that may result in what’s known\n",
    "as the “small file problem”—many small partition files, introducing an inordinate\n",
    "amount of disk I/O and performance degradation thanks to filesystem operations\n",
    "such as opening, closing, and listing directories, which on a distributed filesystem can\n",
    "be slow.\n",
    "\n",
    "Partitions are also created when you explicitly use certain methods of the DataFrame\n",
    "API. For example, while creating a large DataFrame or reading a large file from disk,\n",
    "you can explicitly instruct Spark to create a certain number of partitions\n",
    "\n",
    "// In Scala\n",
    "val ds = spark.read.textFile(\"../README.md\").repartition(16)\n",
    "ds: org.apache.spark.sql.Dataset[String] = [value: string]\n",
    "ds.rdd.getNumPartitions\n",
    "res5: Int = 16\n",
    "val numDF = spark.range(1000L * 1000 * 1000).repartition(16)\n",
    "numDF.rdd.getNumPartitions\n",
    "numDF: org.apache.spark.sql.Dataset[Long] = [id: bigint]\n",
    "res12: Int = 16\n",
    "\n",
    "Finally, shuffle partitions are created during the shuffle stage. By default, the number\n",
    "of shuffle partitions is set to 200 in spark.sql.shuffle.partitions. You can adjust\n",
    "this number depending on the size of the data set you have, to reduce the amount of\n",
    "small partitions being sent across the network to executors’ tasks\n",
    "\n",
    "**The default value for spark.sql.shuffle.partitions is too high\n",
    "for smaller or streaming workloads; you may want to reduce it to a\n",
    "lower value such as the number of cores on the executors or less.**\n",
    "\n",
    "Created during operations like groupBy() or join(), also known as wide transformations,\n",
    "shuffle partitions consume both network and disk I/O resources. During these\n",
    "operations, the shuffle will spill results to executors’ local disks at the location specified\n",
    "in `spark.local.directory`. Having performant SSD disks for this operation will\n",
    "boost the performance.\n",
    "\n",
    "There is no magic formula for the number of shuffle partitions to set for the shuffle\n",
    "stage; the number may vary depending on your use case, data set, number of cores,\n",
    "and the amount of executor memory available—it’s a trial-and-error approach\n",
    "\n",
    "In addition to scaling Spark for large workloads, to boost your performance you’ll\n",
    "want to consider caching or persisting your frequently accessed DataFrames or tables.\n",
    "We explore various caching and persistence options in the next section.\n",
    "\n",
    "## Caching and Persistence of Data\n",
    "What is the difference between caching and persistence? In Spark they are synonymous.\n",
    "Two API calls, `cache()` and `persist()`, offer these capabilities. The latter provides\n",
    "more control over how and where your data is stored—in memory and on disk,\n",
    "serialized and unserialized. Both contribute to better performance for frequently\n",
    "accessed DataFrames or tables.\n",
    "\n",
    "#### DataFrame.cache()\n",
    "cache() will store as many of the partitions read in memory across Spark executors\n",
    "as memory allows (see Figure 7-2). While a DataFrame may be fractionally cached,\n",
    "partitions cannot be fractionally cached (e.g., if you have 8 partitions but only 4.5\n",
    "partitions can fit in memory, only 4 will be cached). However, if not all your partitions\n",
    "are cached, when you want to access the data again, the partitions that are not\n",
    "cached will have to be recomputed, slowing down your Spark job.\n",
    "Let’s look at an example of how caching a large DataFrame improves performance\n",
    "when accessing a DataFrame:\n",
    "\n",
    "```\n",
    "// In Scala\n",
    "// Create a DataFrame with 10M records\n",
    "val df = spark.range(1 * 10000000).toDF(\"id\").withColumn(\"square\", $\"id\" * $\"id\")\n",
    "df.cache() // Cache the data\n",
    "df.count() // Materialize the cache\n",
    "res3: Long = 10000000\n",
    "Command took 5.11 seconds\n",
    "df.count() // Now get it from the cache\n",
    "res4: Long = 10000000\n",
    "Command took 0.44 seconds\n",
    "```\n",
    "The first count() materializes the cache, whereas the second one accesses the cache,\n",
    "resulting in a close to 12 times faster access time for this data set.\n",
    "When you use cache() or persist(), the DataFrame is not fully\n",
    "cached until you invoke an action that goes through every record\n",
    "(e.g., count()). If you use an action like take(1), only one partition\n",
    "will be cached because Catalyst realizes that you do not need\n",
    "to compute all the partitions just to retrieve one record.\n",
    "\n",
    "Observing how a DataFrame is stored across one executor on a local host, as displayed\n",
    "in Figure 7-4, we can see they all fit in memory (recall that at a low level Data‐\n",
    "Frames are backed by RDDs).\n",
    "\n",
    "#### DataFrame.persist()\n",
    "persist(StorageLevel.LEVEL) is nuanced, providing control over how your data is\n",
    "cached via StorageLevel. Table 7-2 summarizes the different storage levels. Data on\n",
    "disk is always serialized using either Java or Kryo serialization.\n",
    "\n",
    "\n",
    "|StorageLevel| Description|\n",
    "|----|----|\n",
    "|MEMORY_ONLY| Data is stored directly as objects and stored only in memory.|\n",
    "|MEMORY_ONLY_SER |Data is serialized as compact byte array representation and stored only in memory. To use it, it has to be deserialized at a cost.|\n",
    "|MEMORY_AND_DISK |Data is stored directly as objects in memory, but if there’s insufficient memory the rest is serialized and stored on disk.|\n",
    "|DISK_ONLY| Data is serialized and stored on disk.|\n",
    "|OFF_HEAP| Data is stored off-heap. Off-heap memory is used in Spark for storage and query execution; see “Configuring Spark executors’ memory and the shuffle service” on page 178.|\n",
    "|MEMORY_AND_DISK_SER |Like MEMORY_AND_DISK, but data is serialized when stored in memory. (Data is always serialized when stored on disk.)|\n",
    "\n",
    "**note** \n",
    "== Each StorageLevel (except OFF_HEAP) has an equivalent LEVEL_NAME_2, which means replicate twice on two different Spark executors: MEMORY_ONLY_2, MEMORY_AND_DISK_SER_2, etc. While this option is expensive, it allows data locality in two places, providing fault tolerance and giving Spark the option to schedule a task local to a copy of the data ==\n",
    "\n",
    "Let’s look at the same example as in the previous section, but using the persist()\n",
    "method:\n",
    "```\n",
    "// In Scala\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "// Create a DataFrame with 10M records\n",
    "val df = spark.range(1 * 10000000).toDF(\"id\").withColumn(\"square\", $\"id\" * $\"id\")\n",
    "df.persist(StorageLevel.DISK_ONLY) // Serialize the data and cache it on disk\n",
    "df.count() // Materialize the cache\n",
    "res2: Long = 10000000\n",
    "Command took 2.08 seconds\n",
    "df.count() // Now get it from the cache\n",
    "res3: Long = 10000000\n",
    "Command took 0.38 seconds\n",
    "```\n",
    "As you can see from Figure 7-5, the data is persisted on disk, not in memory. To\n",
    "unpersist your cached data, just call `DataFrame.unpersist()`.\n",
    "\n",
    "Finally, not only can you cache DataFrames, but you can also cache the tables or\n",
    "views derived from DataFrames. This gives them more readable names in the Spark\n",
    "UI. For example:\n",
    "```\n",
    "// In Scala\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\"CACHE TABLE dfTable\")\n",
    "spark.sql(\"SELECT count(*) FROM dfTable\").show()\n",
    "+--------+\n",
    "|count(1)|\n",
    "+--------+\n",
    "|10000000|\n",
    "+--------+\n",
    "Command took 0.56 seconds\n",
    "```\n",
    "\n",
    "#### When to Cache and Persist\n",
    "Common use cases for caching are scenarios where you will want to access a large\n",
    "data set repeatedly for queries or transformations. Some examples include:\n",
    "• DataFrames commonly used during iterative machine learning training\n",
    "• DataFrames accessed commonly for doing frequent transformations during ETL\n",
    "or building data pipelines\n",
    "\n",
    "#### When Not to Cache and Persist\n",
    "Not all use cases dictate the need to cache. Some scenarios that may not warrant caching\n",
    "your DataFrames include:\n",
    "• DataFrames that are too big to fit in memory\n",
    "• An inexpensive transformation on a DataFrame not requiring frequent use,\n",
    "regardless of size\n",
    "As a general rule you should use memory caching judiciously, as it can incur resource\n",
    "costs in serializing and deserializing, depending on the StorageLevel used.\n",
    "\n",
    "## A Family of Spark Joins\n",
    "\n",
    "Spark has five distinct join strategies by which it exchanges, moves, sorts, groups, and\n",
    "merges data across executors: the \n",
    "```\n",
    "broadcast hash join (BHJ), \n",
    "shuffle hash join (SHJ),\n",
    "shuffle sort merge join (SMJ), \n",
    "broadcast nested loop join (BNLJ), and \n",
    "shuffle-andreplicatednested loop join (a.k.a. Cartesian product join)\n",
    "```\n",
    "We’ll focus on only two of\n",
    "these here (BHJ and SMJ), because they’re the most common ones you’ll encounter\n",
    "\n",
    "\n",
    "### Broadcast Hash Join\n",
    "Also known as a map-side-only join, the broadcast hash join is employed when two\n",
    "data sets, one small (fitting in the driver’s and executor’s memory) and another large\n",
    "enough to ideally be spared from movement, need to be joined over certain conditions\n",
    "or columns. Using a Spark broadcast variable, the smaller data set is broadcasted\n",
    "by the driver to all Spark executors, as shown in Figure 7-6, and subsequently\n",
    "joined with the larger data set on each executor. This strategy avoids the large\n",
    "exchange.\n",
    "\n",
    "By default Spark will use a broadcast join if the smaller data set is less than 10 MB.\n",
    "This configuration is set in `spark.sql.autoBroadcastJoinThreshold`; you can\n",
    "decrease or increase the size depending on how much memory you have on each\n",
    "executor and in the driver. If you are confident that you have enough memory you\n",
    "can use a broadcast join with DataFrames larger than 10 MB (even up to 100 MB).\n",
    "\n",
    "For example, consider a simple case where you have a large data set of soccer\n",
    "players around the world, playersDF, and a smaller data set of soccer clubs they play\n",
    "for, clubsDF, and you wish to join them over a common key:\n",
    "```\n",
    "// In Scala\n",
    "import org.apache.spark.sql.functions.broadcast\n",
    "val joinedDF = playersDF.join(broadcast(clubsDF), \"key1 === key2\")\n",
    "```\n",
    "note: *In this code we are forcing Spark to do a broadcast join, but it will\n",
    "resort to this type of join by default if the size of the smaller data set\n",
    "is below the spark.sql.autoBroadcastJoinThreshold.*\n",
    "\n",
    "The BHJ is the easiest and fastest join Spark offers, since it does not involve any shuffle\n",
    "of the data set; all the data is available locally to the executor after a broadcast. You\n",
    "just have to be sure that you have enough memory both on the Spark driver’s and the\n",
    "executors’ side to hold the smaller data set in memory.\n",
    "At any time after the operation, you can see in the physical plan what join operation\n",
    "was performed by executing:\n",
    "\n",
    "`joinedDF.explain(mode)`\n",
    "\n",
    "#### When to use a broadcast hash join\n",
    "\n",
    "Use this type of join under the following conditions for maximum benefit:\n",
    "• When each key within the smaller and larger data sets is hashed to the same partition\n",
    "by Spark\n",
    "• When one data set is much smaller than the other (and within the default config\n",
    "of 10 MB, or more if you have sufficient memory)\n",
    "• When you only want to perform an equi-join, to combine two data sets based on\n",
    "matching unsorted keys\n",
    "• When you are not worried by excessive network bandwidth usage or OOM\n",
    "errors, because the smaller data set will be broadcast to all Spark executors\n",
    "Specifying a value of `-1 in spark.sql.autoBroadcastJoinThreshold` will cause\n",
    "Spark to always resort to a shuffle sort merge join, which we discuss in the next\n",
    "section.\n",
    "\n",
    "\n",
    "#### Shuffle Sort Merge Join\n",
    "The sort-merge algorithm is an efficient way to merge two large data sets over a common\n",
    "key that is sortable, unique, and can be assigned to or stored in the same partition—\n",
    "that is, two data sets with a common hashable key that end up being on the\n",
    "same partition. From Spark’s perspective, this means that all rows within each data set\n",
    "with the same key are hashed on the same partition on the same executor. Obviously,\n",
    "this means data has to be colocated or exchanged between executors.\n",
    "\n",
    "##### Optimizing the shuffle sort merge join\n",
    "We can eliminate the Exchange step from this scheme if we create partitioned buckets\n",
    "for common sorted keys or columns on which we want to perform frequent equijoins.\n",
    "That is, we can create an explicit number of buckets to store specific sorted columns\n",
    "(one key per bucket). Presorting and reorganizing data in this way boosts\n",
    "performance, as it allows us to skip the expensive Exchange operation and go straight\n",
    "to WholeStageCodegen.\n",
    "\n",
    "##### When to use a shuffle sort merge join\n",
    "Use this type of join under the following conditions for maximum benefit:\n",
    "• When each key within two large data sets can be sorted and hashed to the same\n",
    "partition by Spark\n",
    "• When you want to perform only equi-joins to combine two data sets based on\n",
    "matching sorted keys\n",
    "• When you want to prevent Exchange and Sort operations to save large shuffles\n",
    "across the network\n",
    "So far we have covered operational aspects related to tuning and optimizing Spark,\n",
    "and how Spark exchanges data during two common join operations. We also demonstrated\n",
    "how you can boost the performance of a shuffle sort merge join operation by\n",
    "using bucketing to avoid large exchanges of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf8abd-0af0-498b-9a67-390dd44a15b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a58711-3834-4413-ba4c-df28a02fcb12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43851f-9d38-4848-bfd9-c6d863406c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896ae82-da24-412b-881a-323aeba33e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748544a-16fc-44bc-9707-abece70a5863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae0ef5-684a-466a-a0f1-ef2f32f31e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87dfd89-397f-4e5e-b0b0-2178b6729764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa4693-40aa-4368-b53b-596e14128656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e1716-9441-4005-97a6-4f170e4a3fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb75b44-b3fd-4c66-b875-71ef1cee35a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464309a-e405-4c51-88a6-9d57521fbb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1712e-8c43-4b8a-8786-267c5e78ccff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6535ec-47e5-4b94-af7b-3baec11006f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19181b-b456-4ec2-9faa-6404f6df869d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e497d241-2e7f-4552-8d94-c82e230993ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ca794-77d8-4af0-8a0b-a17df9fa0199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153c47c-fc13-4406-8984-8fca81b698c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ecea2-c1c8-4a03-a9cf-d87088b768bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f33867-2d03-4762-a63e-3c4632066775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
