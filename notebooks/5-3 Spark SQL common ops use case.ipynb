{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecaa62a0-0844-4f7c-9431-b6d34393ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f, types as t\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3021918c-9236-4e47-925d-63a30d82bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_dataset = os.path.join(os.getcwd(),\"datasets/airport-codes-na.txt\")\n",
    "delays_dataset = os.path.join(os.getcwd(),\"datasets/departuredelays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54bdfd2-750b-4fc5-b8a5-d60d46a95a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/17 21:45:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"spark-sql\").config(\"spark.driver.bindAddress\",\"127.0.0.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2fe53d-3125-41cb-a655-0caed8307d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport_codes = spark.read\\\n",
    "                        .format(\"csv\")\\\n",
    "                        .option(\"header\",\"true\")\\\n",
    "                        .option(\"delimiter\",\"\\t\")\\\n",
    "                        .load(airport_codes_dataset)\n",
    "\n",
    "df_delays = spark.read\\\n",
    "                        .format(\"csv\")\\\n",
    "                        .option(\"header\",\"true\")\\\n",
    "                        .load(delays_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc404381-4278-40df-9f27-04275d489e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "|   Alliance|   NE|    USA| AIA|\n",
      "|     Alpena|   MI|    USA| APN|\n",
      "|    Altoona|   PA|    USA| AOO|\n",
      "|   Amarillo|   TX|    USA| AMA|\n",
      "|Anahim Lake|   BC| Canada| YAA|\n",
      "|  Anchorage|   AK|    USA| ANC|\n",
      "|   Appleton|   WI|    USA| ATW|\n",
      "|     Arviat|  NWT| Canada| YEK|\n",
      "|  Asheville|   NC|    USA| AVL|\n",
      "|      Aspen|   CO|    USA| ASE|\n",
      "+-----------+-----+-------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "|01061215|   -6|     602|   ABE|        ATL|\n",
      "|01061725|   69|     602|   ABE|        ATL|\n",
      "|01061230|    0|     369|   ABE|        DTW|\n",
      "|01060625|   -3|     602|   ABE|        ATL|\n",
      "|01070600|    0|     369|   ABE|        DTW|\n",
      "|01071725|    0|     602|   ABE|        ATL|\n",
      "|01071230|    0|     369|   ABE|        DTW|\n",
      "|01070625|    0|     602|   ABE|        ATL|\n",
      "|01071219|    0|     569|   ABE|        ORD|\n",
      "|01080600|    0|     369|   ABE|        DTW|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_codes.show()\n",
    "df_delays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2639d79a-8648-45ab-a366-83a007ea16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using expr(), convert the delay and distance columns from STRING to INT.\n",
    "\n",
    "df_delays = df_delays\\\n",
    "                    .withColumn(\"delay\", f.expr(\"cast(delay as INT) as delay\"))\\\n",
    "                    .withColumn(\"distance\", f.expr(\"cast(distance as INT) as distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8eafe36-a238-470f-9a13-c04210a4b5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delays.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d5d94f-caa1-4fbb-a321-7e51310115ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller table, foo, that we can focus on for our demo examples; it contains\n",
    "# only information on three flights originating from Seattle (SEA) to the destination\n",
    "# of San Francisco (SFO) for a small time range.\n",
    "\n",
    "df_filtered = df_delays.where(f.expr(\"\"\"origin == 'SEA' and destination == 'SFO' and \n",
    "                        delay > 0 and date like '01010%'\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eddeeb-396b-46e2-9999-b3e794df0e9d",
   "metadata": {},
   "source": [
    "## Unions\n",
    "A common pattern within Apache Spark is to union two different DataFrames with\n",
    "the same schema together. This can be achieved using the `union()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0700697c-7cb7-459b-ac0c-4e63982ebe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union = df_delays.union(df_filtered)\n",
    "\n",
    "df_union.createOrReplaceTempView(\"union_data\")\n",
    "\n",
    "df_union.filter(f.expr(\"\"\" origin == 'SEA' and destination == 'SFO' and \n",
    "                        delay > 0 and date like '01010%' \"\"\")).show()\n",
    "\n",
    "# The bar DataFrame is the union of foo with delays. Using the same filtering criteria\n",
    "# results in the bar DataFrame, we see a duplication of the foo data, as expected:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be9398-8536-4d27-abff-fdcd2eea3181",
   "metadata": {},
   "source": [
    "## Joins\n",
    "A common DataFrame operation is to join two DataFrames (or tables) together. By\n",
    "default, a Spark SQL join is an inner join, with the options being inner, cross,\n",
    "outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and\n",
    "left_anti. More information is available in the documentation (this is applicable to\n",
    "Scala as well as Python).\n",
    "\n",
    "The following code sample performs the default of an inner join between the air\n",
    "portsna and foo DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc85bf58-6099-4bc9-8958-e0c1ae5712ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = df_airport_codes.join(df_filtered, on=df_filtered.origin == df_airport_codes.IATA)\\\n",
    "                            .select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b80b2-2df2-4a8c-bd58-15a8046d85c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16325d1a-6d70-47c7-88c0-0af161947a92",
   "metadata": {},
   "source": [
    "# Windowing\n",
    "\n",
    "A window function uses values from the rows in a window (a range of input rows) to\n",
    "return a set of values, typically in the form of another row. With window functions, it\n",
    "is possible to operate on a group of rows while still returning a single value for every\n",
    "input row. In this section, we will show how to use the dense_rank() window function;\n",
    "there are many other functions, as noted in Table 5-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc1798a-0e6a-4b8e-a1f0-8c9e94fe8e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================================>              (6 + 2) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|origin|destination|total_delay|\n",
      "+------+-----------+-----------+\n",
      "|   JFK|        LAX|      35755|\n",
      "|   JFK|        SFO|      35619|\n",
      "|   JFK|        MCO|      28419|\n",
      "|   SEA|        SFO|      22293|\n",
      "|   SEA|        DEN|      13645|\n",
      "|   SEA|        ORD|      10041|\n",
      "|   SFO|        LAX|      40798|\n",
      "|   SFO|        LAS|      30030|\n",
      "|   SFO|        ORD|      27412|\n",
      "+------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# What if for each of these origin airports you wanted to find the three destinations that experienced the most delays?\n",
    "\n",
    "\n",
    "df_delays.createOrReplaceTempView(\"delays\")\n",
    "\n",
    "spark.sql(\"\"\" select origin, destination, total_delay from\n",
    "            (select origin, destination, total_delay, dense_rank() over (partition by origin order by total_delay desc) as drank\n",
    "               from (select origin, destination, sum(delay) as total_delay from delays group by origin, destination) q\n",
    "                ) o \n",
    "                where drank <= 3\n",
    "                and origin in ('SEA','SFO','JFK')\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde80a21-c116-434f-a02b-c535dee9def1",
   "metadata": {},
   "source": [
    "### need to learn about pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69e078-467a-4b10-874f-fcab927f1b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abc658-008b-4f59-b54f-24941a972d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77fdf3-197f-49d6-9fe0-4127a0ab11d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c539056-a797-46e3-bca9-eb1eac55cc63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee1939-d7dc-470d-9c4a-9c8e61603c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ee254-9ddb-434e-809d-f5f244b6e0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
