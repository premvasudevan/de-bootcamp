{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5852dcf-429c-4853-8cfe-c5b9dca4bee6",
   "metadata": {},
   "source": [
    "# User-Defined Functions\n",
    "While Apache Spark has a plethora of built-in functions, the flexibility of Spark\n",
    "allows for data engineers and data scientists to define their own functions too. These\n",
    "are known as user-defined functions (UDFs).\n",
    "\n",
    "## Spark SQL UDFs\n",
    "The benefit of creating your own PySpark or Scala UDFs is that you (and others) will\n",
    "be able to make use of them within Spark SQL itself. For example, a data scientist can\n",
    "wrap an ML model within a UDF so that a data analyst can query its predictions in\n",
    "Spark SQL without necessarily understanding the internals of the model.\n",
    "Here’s a simplified example of creating a Spark SQL UDF. Note that UDFs operate per\n",
    "session and they will not be persisted in the underlying metastore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c0a0ef-577e-4dcf-b229-7fdd08989b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e8b624-4558-44f1-b658-744127b7b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/07 20:05:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"udf\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b03efd-158b-4b08-ab43-64abfae7329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubeFunc(val):\n",
    "    return val ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5561972c-84d2-443f-8d25-714b63d327c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cubeFunc(val)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"cubed\", cubeFunc, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78699b6a-1017-4e83-b614-6e00c2d69dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,10).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b2925a-74ca-463b-a837-3af6bdf3a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                        (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|cubed(id)|\n",
      "+---+---------+\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "|  4|       64|\n",
      "|  5|      125|\n",
      "|  6|      216|\n",
      "|  7|      343|\n",
      "|  8|      512|\n",
      "|  9|      729|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, cubed(id) from udf_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6199bea3-1de9-481b-9c1a-16d2fad8d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectangle_area(w,h):\n",
    "    return w * h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fdf186a-ae93-4771-aebc-2bf81d16eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select id as width, cubed(id) as height from udf_test\").createOrReplaceTempView(\"rectangle_dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e49ef306-1732-4c7e-8fb1-4971076e2405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.rectangle_area(w, h)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"calc_rectangle_area\",rectangle_area, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d5346ad-1cf4-4381-b91e-41b6e5298d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------------------------------+\n",
      "|width|height|calc_rectangle_area(width, height)|\n",
      "+-----+------+----------------------------------+\n",
      "|    1|     1|                                 1|\n",
      "|    2|     8|                                16|\n",
      "|    3|    27|                                81|\n",
      "|    4|    64|                               256|\n",
      "|    5|   125|                               625|\n",
      "|    6|   216|                              1296|\n",
      "|    7|   343|                              2401|\n",
      "|    8|   512|                              4096|\n",
      "|    9|   729|                              6561|\n",
      "+-----+------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select width, height, calc_rectangle_area(width, height) from rectangle_dimensions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b6b1d-25f7-45cf-a167-042d832a1f6c",
   "metadata": {},
   "source": [
    "## Evaluation order and null checking in Spark SQL\n",
    "Spark SQL (this includes SQL, the DataFrame API, and the Dataset API) does not\n",
    "guarantee the order of evaluation of subexpressions. For example, the following query\n",
    "does not guarantee that the s is NOT NULL clause is executed prior to the strlen(s)> 1 clause:\n",
    "\n",
    "spark.sql(\"SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) > 1\")\n",
    "\n",
    "Therefore, to perform proper null checking, it is recommended that you do the\n",
    "following:\n",
    "1. Make the UDF itself null-aware and do null checking inside the UDF.\n",
    "2. Use IF or CASE WHEN expressions to do the null check and invoke the UDF in a\n",
    "conditional branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f13a53-a301-48ea-b606-d23ede3d9864",
   "metadata": {},
   "source": [
    "## Speeding up and distributing PySpark UDFs with Pandas UDFs\n",
    "One of the previous prevailing issues with using PySpark UDFs was that they had\n",
    "slower performance than Scala UDFs. This was because the PySpark UDFs required\n",
    "data movement between the JVM and Python, which was quite expensive. To resolve\n",
    "this problem, Pandas UDFs (also known as vectorized UDFs) were introduced as part\n",
    "of Apache Spark 2.3. A Pandas UDF uses Apache Arrow to transfer data and Pandas\n",
    "to work with the data. You define a Pandas UDF using the keyword pandas_udf as\n",
    "the decorator, or to wrap the function itself. Once the data is in Apache Arrow format,\n",
    "there is no longer the need to serialize/pickle the data as it is already in a format\n",
    "consumable by the Python process. Instead of operating on individual inputs row by\n",
    "row, you are operating on a Pandas Series or DataFrame (i.e., vectorized execution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e601b-a968-4d74-87b8-d1eb70e5b11f",
   "metadata": {},
   "source": [
    "From Apache Spark 3.0 with Python 3.6 and above, Pandas UDFs were split into two\n",
    "API categories: Pandas UDFs and Pandas Function APIs.\n",
    "\n",
    "## Pandas UDFs\n",
    "With Apache Spark 3.0, Pandas UDFs infer the Pandas UDF type from Python\n",
    "type hints in Pandas UDFs such as pandas.Series, pandas.DataFrame, Tuple,\n",
    "and Iterator. Previously you needed to manually define and specify each Pandas\n",
    "UDF type. Currently, the supported cases of Python type hints in Pandas\n",
    "UDFs are Series to Series, Iterator of Series to Iterator of Series, Iterator of Multiple\n",
    "Series to Iterator of Series, and Series to Scalar (a single value).\n",
    "\n",
    "## Pandas Function APIs\n",
    "Pandas Function APIs allow you to directly apply a local Python function to a\n",
    "PySpark DataFrame where both the input and output are Pandas instances. For\n",
    "Spark 3.0, the supported Pandas Function APIs are grouped map, map, cogrouped\n",
    "map.\n",
    "\n",
    "For more information, refer to “Redesigned Pandas UDFs with Python Type Hints”\n",
    "on page 354 in Chapter 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f526d5c-c0b1-49b7-b004-42d107f3a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8543e9a-fc52-4b1f-8fc4-1a3464b19d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube_udf(n: pd.Series) -> pd.Series:\n",
    "    return n ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "561d46f9-a6d2-47e8-81cf-2fc531f2d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow\n",
    "get_cube_of = pandas_udf(cube_udf, LongType())\n",
    "\n",
    "# above line of code depends on pyarrow, error will be thrown if not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f2c1a1b-d6d0-4ee7-9fef-2bdc9bfb41fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nums = pd.Series([1,2,3])\n",
    "print(cube_udf(nums))    # this is usual pandas function not pandas udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5adc4bc-188f-45d3-a4bf-2e6684c0cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                        (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|cube_udf(id)|\n",
      "+---+------------+\n",
      "|  1|           1|\n",
      "|  2|           8|\n",
      "|  3|          27|\n",
      "+---+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Now let’s switch to a Spark DataFrame. We can execute this function as a Spark vectorized UDF as follows:\n",
    "\n",
    "df = spark.range(1,4)\n",
    "df.select(\"id\", get_cube_of(\"id\")).show() # pandas udf is used here for better performance \n",
    "\n",
    "\n",
    "# As opposed to a local function, using a vectorized UDF will result in the execution of\n",
    "# Spark jobs; the previous local function is a Pandas function executed only on the\n",
    "# Spark driver. This becomes more apparent when viewing the Spark UI for one of the\n",
    "# stages of this pandas_udf function (Figure 5-1).\n",
    "# For a deeper dive into Pandas UDFs, refer to pandas user-defined\n",
    "# functions documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
