{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76516be-f3e6-4ff6-bb2a-d7c6a18041e7",
   "metadata": {},
   "source": [
    "# notes\n",
    "## intro\n",
    "At a programmatic level, Spark SQL allows developers to issue ANSI SQL:2003–compatible\n",
    "queries on structured data with a schema. Since its introduction in Spark 1.3,\n",
    "Spark SQL has evolved into a substantial engine upon which many high-level structured\n",
    "functionalities have been built. Apart from allowing you to issue SQL-like queries\n",
    "on your data, the Spark SQL engine:\n",
    "• Unifies Spark components and permits abstraction to DataFrames/Datasets in\n",
    "Java, Scala, Python, and R, which simplifies working with structured data sets.\n",
    "• Connects to the Apache Hive metastore and tables.\n",
    "• Reads and writes structured data with a specific schema from structured file formats\n",
    "(JSON, CSV, Text, Avro, Parquet, ORC, etc.) and converts data into temporary\n",
    "tables.\n",
    "• Offers an interactive Spark SQL shell for quick data exploration.\n",
    "• Provides a bridge to (and from) external tools via standard database JDBC/\n",
    "ODBC connectors.\n",
    "• Generates optimized query plans and compact code for the JVM, for final\n",
    "execution.\n",
    "\n",
    "---------------\n",
    "## The Catalyst Optimizer\n",
    "The Catalyst optimizer takes a computational query and converts it into an execution\n",
    "plan. It goes through four transformational phases, as shown in Figure 3-4:\n",
    "1. Analysis\n",
    "2. Logical optimization\n",
    "3. Physical planning\n",
    "4. Code generation\n",
    "\n",
    "To see the different stages the Python code goes through, you can use the\n",
    "count_mnm_df.explain(True) method on the DataFrame. Or, to get a look at the different\n",
    "logical and physical plans, in Scala you can call df.queryExecution.logical\n",
    "or df.queryExecution.optimizedPlan.\n",
    "\n",
    "## Using Spark SQL in Spark Applications\n",
    "The SparkSession, introduced in Spark 2.0, provides a unified entry point for programming\n",
    "Spark with the Structured APIs. You can use a SparkSession to access\n",
    "Spark functionality: just import the class and create an instance in your code.\n",
    "To issue any SQL query, use the sql() method on the SparkSession instance, spark,\n",
    "such as spark.sql(\"SELECT * FROM myTableName\").\n",
    "\n",
    "# refs\n",
    "https://stackoverflow.com/questions/56895707/pyspark-difference-performance-for-spark-read-formatcsv-vs-spark-read-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212305ea-a955-482f-a161-6ccf73abf654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/06 00:06:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('spark-sql').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20519f74-2018-4a8e-abfe-5e4a99df4def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/departuredelays.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "flight_delay_ds_path = os.path.join(os.getcwd(),\"datasets/departuredelays.csv\")\n",
    "print(flight_delay_ds_path)\n",
    "flight_delay_df = spark.read\\\n",
    "                    .format(\"csv\")\\\n",
    "                    .option(\"inferSchema\",\"true\")\\\n",
    "                    .option(\"header\",\"true\")\\\n",
    "                    .load(flight_delay_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93628d36-ae3c-4954-8870-a4e4ee4bfe05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|date   |delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|6    |602     |ABE   |ATL        |\n",
      "|1020600|-8   |369     |ABE   |DTW        |\n",
      "|1021245|-2   |602     |ABE   |ATL        |\n",
      "|1020605|-4   |602     |ABE   |ATL        |\n",
      "|1031245|-4   |602     |ABE   |ATL        |\n",
      "|1030605|0    |602     |ABE   |ATL        |\n",
      "|1041243|10   |602     |ABE   |ATL        |\n",
      "|1040605|28   |602     |ABE   |ATL        |\n",
      "|1051245|88   |602     |ABE   |ATL        |\n",
      "|1050605|9    |602     |ABE   |ATL        |\n",
      "|1061215|-6   |602     |ABE   |ATL        |\n",
      "|1061725|69   |602     |ABE   |ATL        |\n",
      "|1061230|0    |369     |ABE   |DTW        |\n",
      "|1060625|-3   |602     |ABE   |ATL        |\n",
      "|1070600|0    |369     |ABE   |DTW        |\n",
      "|1071725|0    |602     |ABE   |ATL        |\n",
      "|1071230|0    |369     |ABE   |DTW        |\n",
      "|1070625|0    |602     |ABE   |ATL        |\n",
      "|1071219|0    |569     |ABE   |ORD        |\n",
      "|1080600|0    |369     |ABE   |DTW        |\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('date', StringType(), True), StructField('delay', IntegerType(), True), StructField('distance', IntegerType(), True), StructField('origin', StringType(), True), StructField('destination', StringType(), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flight_delay_df.schema\n",
    "flight_delay_df.show(truncate=False)\n",
    "\n",
    "# infer schema caused the date to be int but we need it as string\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import cast, col\n",
    "\n",
    "flight_delay_df = flight_delay_df.withColumn('date',col('date').cast(\"string\"))\n",
    "flight_delay_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db1b7723-507f-4809-8cec-857c34e6dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp view\n",
    "\n",
    "flight_delay_df.createOrReplaceTempView(\"flight_delay_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2104669f-b190-41bb-b447-ce4296254fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+\n",
      "|origin|destination|distance|\n",
      "+------+-----------+--------+\n",
      "|   HNL|        JFK|    4330|\n",
      "|   JFK|        HNL|    4330|\n",
      "|   EWR|        HNL|    4312|\n",
      "|   HNL|        EWR|    4312|\n",
      "|   HNL|        IAD|    4186|\n",
      "|   IAD|        HNL|    4186|\n",
      "|   ATL|        HNL|    3912|\n",
      "|   HNL|        ATL|    3912|\n",
      "|   HNL|        ORD|    3687|\n",
      "|   ORD|        HNL|    3687|\n",
      "|   IAH|        HNL|    3392|\n",
      "|   HNL|        IAH|    3392|\n",
      "|   HNL|        GUM|    3303|\n",
      "|   GUM|        HNL|    3303|\n",
      "|   HNL|        DFW|    3288|\n",
      "|   DFW|        HNL|    3288|\n",
      "|   DFW|        OGG|    3224|\n",
      "|   OGG|        DFW|    3224|\n",
      "|   DEN|        LIH|    2967|\n",
      "|   LIH|        DEN|    2967|\n",
      "+------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        select distinct origin, destination, distance from flight_delay_view \n",
    "        where distance > 1000 \n",
    "        order by distance desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0073c0-9451-4fde-8c95-e7b09e31dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+-----+\n",
      "|   date|origin|destination|delay|\n",
      "+-------+------+-----------+-----+\n",
      "|2190925|   SFO|        ORD| 1638|\n",
      "|1031755|   SFO|        ORD|  396|\n",
      "|1022330|   SFO|        ORD|  326|\n",
      "|1051205|   SFO|        ORD|  320|\n",
      "|1190925|   SFO|        ORD|  297|\n",
      "|2171115|   SFO|        ORD|  296|\n",
      "|1071040|   SFO|        ORD|  279|\n",
      "|1051550|   SFO|        ORD|  274|\n",
      "|3120730|   SFO|        ORD|  266|\n",
      "|1261104|   SFO|        ORD|  258|\n",
      "|1161210|   SFO|        ORD|  225|\n",
      "|2091800|   SFO|        ORD|  223|\n",
      "|1221040|   SFO|        ORD|  215|\n",
      "|3121155|   SFO|        ORD|  203|\n",
      "|2111256|   SFO|        ORD|  197|\n",
      "|3311405|   SFO|        ORD|  196|\n",
      "|1031920|   SFO|        ORD|  193|\n",
      "|1021410|   SFO|        ORD|  190|\n",
      "|3171215|   SFO|        ORD|  189|\n",
      "|1101410|   SFO|        ORD|  184|\n",
      "+-------+------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find all flights between San Francisco (SFO) and Chicago (ORD) with at least a two-hour delay\n",
    "\n",
    "spark.sql(\"\"\"select date, origin, destination, delay from flight_delay_view\n",
    "            where origin=\"SFO\" and destination = \"ORD\" and delay >= 120\n",
    "            order by delay desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdaa9a0-f022-4378-af97-b7d8c2a8ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an exercise, convert the date column into a readable format and find\n",
    "# the days or months when these delays were most common. Were the delays related to\n",
    "# winter months or holidays?)\n",
    "\n",
    "from pyspark.sql.functions import to_date, to_timestamp, concat, lit, when, length\n",
    "\n",
    "flight_delay_df = flight_delay_df.withColumn('date_formatted', to_timestamp(when(length('date') == 8, (concat(lit('2024'),col('date'))))\\\n",
    "                                                                .otherwise((concat(lit('20240'),col('date')))), 'yyyyMMddHHmm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529b862-b15a-4a23-a142-dfc7639fd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_delay_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf53a1-cb87-42d2-a910-1e4488673814",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_delay_df.createOrReplaceTempView(\"flight_delay_formatted_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45578e24-da98-4221-a7b7-7820ab2b5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select month(date_formatted), day(date_formatted), count(1) as num_of_delays, (sum(delay)/60) total_delayed_hrs from flight_delay_formatted_date\n",
    "            where delay > 0\n",
    "            group by month(date_formatted), day(date_formatted)\n",
    "            order by 3 desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e3df5-c610-4279-898a-35a02c1985db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s try a more complicated query where we use the CASE clause in SQL. In the following\n",
    "# example, we want to label all US flights, regardless of origin and destination,\n",
    "# with an indication of the delays they experienced: Very Long Delays (> 6 hours),\n",
    "# Long Delays (2–6 hours), etc. We’ll add these human-readable labels in a new column\n",
    "# called Flight_Delays:\n",
    "\n",
    "spark.sql(\"\"\"select origin, destination, case when delay/60 > 6 then 'very long delay'\n",
    "                                                when delay/60 between 2 and 6 then 'long delay'\n",
    "                                                when delay < 0 then 'arrived early'\n",
    "                                                else 'delay' end as Flight_Delays from flight_delay_formatted_date\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"select case when delay/60 > 6 then 'very long delay'\n",
    "                                                when delay/60 between 2 and 6 then 'long delay'\n",
    "                                                when delay < 0 then 'arrived early'\n",
    "                                                else 'delay' end as Flight_Delays, count(1) from flight_delay_formatted_date\n",
    "                                                group by Flight_Delays order by 2 desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776f2ea-7838-4fa1-b7aa-0d9ef2522ec1",
   "metadata": {},
   "source": [
    "# SQL Tables and Views\n",
    "Tables hold data. Associated with each table in Spark is its relevant metadata, which is\n",
    "information about the table and its data: the schema, description, table name, database\n",
    "name, column names, partitions, physical location where the actual data resides,\n",
    "etc. All of this is stored in a central metastore.\n",
    "\n",
    "Instead of having a separate metastore for Spark tables, Spark by default uses the\n",
    "Apache Hive metastore, located at /user/hive/warehouse, to persist all the metadata\n",
    "about your tables. However, you may change the default location by setting the Spark\n",
    "config variable spark.sql.warehouse.dir to another location, which can be set to a\n",
    "local or external distributed storage.\n",
    "\n",
    "# Managed Versus UnmanagedTables\n",
    "Spark allows you to create two types of tables: managed and unmanaged. For a managed\n",
    "table, Spark manages both the metadata and the data in the file store. This could\n",
    "be a local filesystem, HDFS, or an object store such as Amazon S3 or Azure Blob. For\n",
    "an unmanaged table, Spark only manages the metadata, while you manage the data\n",
    "yourself in an external data source such as Cassandra.\n",
    "With a managed table, because Spark manages everything, a SQL command such as\n",
    "DROP TABLE table_name deletes both the metadata and the data. With an unmanaged\n",
    "table, the same command will delete only the metadata, not the actual data. We will\n",
    "look at some examples of how to create managed and unmanaged tables in the next\n",
    "section.\n",
    "\n",
    "# Creating SQL Databases and Tables\n",
    "Tables reside within a database. By default, Spark creates tables under the default\n",
    "database. To create your own database name, you can issue a SQL command from\n",
    "your Spark application or notebook. Using the US flight delays data set, let’s create\n",
    "both a managed and an unmanaged table. To begin, we’ll create a database called\n",
    "learn_spark_db and tell Spark we want to use that database:\n",
    "\n",
    "// In Scala/Python\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")\n",
    "\n",
    "From this point, any commands we issue in our application to create tables will result\n",
    "in the tables being created in this database and residing under the database name\n",
    "learn_spark_db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761057ff-117d-4f53-8105-43c9a2d242ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0106a062-4c2b-498c-84ac-b2e4dcb38bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6724bf1-b4b3-427a-b65d-f9b359d1925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/06 00:09:29 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n'CreateTable `spark_catalog`.`learn_spark_db`.`managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/lib/python3.8/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/spark/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n'CreateTable `spark_catalog`.`learn_spark_db`.`managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f27decc0-86bd-46e7-b1ae-9435b2d79935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/06 00:09:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(flight_delay_ds_path, schema=schema)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl_new\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbc337-fd41-4334-b635-3128483cd105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0a817-21b8-41d3-b0c2-da0145947d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51911026-5cd3-42e5-9abf-c281aaf6cd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c31efa8-fd7f-4c27-b8d6-6d522c89ea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://localhost:3141/databee/dev/\n",
      "Collecting ipython-sql\n",
      "  Downloading http://localhost:3141/root/prod/%2Bf/61b/46ecffb956f62/ipython_sql-0.5.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython-sql) (1.17.0)\n",
      "Collecting ipython-genutils\n",
      "  Downloading http://localhost:3141/root/prod/%2Bf/72d/d37233799e619/ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=3141): Read timed out. (read timeout=15)\")': /databee/dev/+simple/sqlalchemy/\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=3141): Read timed out. (read timeout=15)\")': /databee/dev/+simple/sqlalchemy/\u001b[0m\u001b[33m\n",
      "Collecting sqlalchemy>=2.0\n",
      "  Downloading http://localhost:3141/root/prod/%2Bf/cf0/e99cdb600eabc/sqlalchemy-2.0.40-cp38-cp38-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m671.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "Requirement already satisfied: ipython in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython-sql) (8.12.0)\n",
      "Collecting prettytable\n",
      "  Downloading http://localhost:3141/root/prod/%2Bf/aa1/7083feb6c71da/prettytable-3.11.0-py3-none-any.whl (28 kB)\n",
      "Collecting sqlparse\n",
      "  Downloading http://localhost:3141/root/prod/%2Bf/cf2/196ed3418f3ba/sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from sqlalchemy>=2.0->ipython-sql) (4.12.2)\n",
      "Requirement already satisfied: stack-data in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (0.6.3)\n",
      "Requirement already satisfied: backcall in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (2.19.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (3.0.50)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (4.9.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (0.19.2)\n",
      "Requirement already satisfied: appnope in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (0.1.4)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (5.14.3)\n",
      "Requirement already satisfied: decorator in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from ipython->ipython-sql) (5.1.1)\n",
      "Requirement already satisfied: wcwidth in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from prettytable->ipython-sql) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\n",
      "Requirement already satisfied: pure-eval in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from stack-data->ipython->ipython-sql) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from stack-data->ipython->ipython-sql) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/pvasud669@apac.comcast.com/spark/lib/python3.8/site-packages (from stack-data->ipython->ipython-sql) (2.2.0)\n",
      "Installing collected packages: ipython-genutils, sqlparse, sqlalchemy, prettytable, ipython-sql\n",
      "Successfully installed ipython-genutils-0.2.0 ipython-sql-0.5.0 prettytable-3.11.0 sqlalchemy-2.0.40 sqlparse-0.5.3\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/pvasud669@apac.comcast.com/spark/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipython-sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eaf40b-6dfa-4bcf-b999-38549fc64320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d4d4608-2fa0-4d69-9c76-84900e3a13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15a3b4e2-4de0-4821-befb-bf5b70716b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01201755|    0|     449|   ORF|        ATL|\n",
      "|01201610|   52|     449|   ORF|        ATL|\n",
      "|01201441|    0|     449|   ORF|        ATL|\n",
      "|01211755|  -15|     449|   ORF|        ATL|\n",
      "|01210941|   -5|     449|   ORF|        ATL|\n",
      "|01210700|    5|     449|   ORF|        ATL|\n",
      "|01211243|    2|     449|   ORF|        ATL|\n",
      "|01210540|   -5|     449|   ORF|        ATL|\n",
      "|01211610|   27|     449|   ORF|        ATL|\n",
      "|01211441|   -6|     449|   ORF|        ATL|\n",
      "|01221755|   -4|     449|   ORF|        ATL|\n",
      "|01220941|   -5|     449|   ORF|        ATL|\n",
      "|01220700|   -2|     449|   ORF|        ATL|\n",
      "|01221243|    0|     449|   ORF|        ATL|\n",
      "|01220540|   -5|     449|   ORF|        ATL|\n",
      "|01221610|   -5|     449|   ORF|        ATL|\n",
      "|01221441|   -2|     449|   ORF|        ATL|\n",
      "|01231755|   -1|     449|   ORF|        ATL|\n",
      "|01230941|   -8|     449|   ORF|        ATL|\n",
      "|01230700|   -3|     449|   ORF|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from managed_us_delay_flights_tbl_new;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b79ff-cfff-4d18-8228-b4a125af8107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38908d9-9f97-473f-b965-d4e148ff8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating an unmanaged table\n",
    "# # By contrast, you can create unmanaged tables from your own data sources—say, Parquet,\n",
    "# # CSV, or JSON files stored in a file store accessible to your Spark application.\n",
    "# # To create an unmanaged table from a data source such as a CSV file, in SQL use:\n",
    "# spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,\n",
    "# distance INT, origin STRING, destination STRING)\n",
    "# USING csv OPTIONS (PATH\n",
    "# '/Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/departuredelays.csv')\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating an unmanaged table\n",
    "# # By contrast, you can create unmanaged tables from your own data sources—say, Parquet,\n",
    "# # CSV, or JSON files stored in a file store accessible to your Spark application.\n",
    "# # To create an unmanaged table from a data source such as a CSV file, in SQL use:\n",
    "# spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,\n",
    "# distance INT, origin STRING, destination STRING)\n",
    "# USING csv OPTIONS (PATH\n",
    "# '/Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/departuredelays.csv')\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2a6b127-a282-4f46-bf88-82414b326caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/06 00:21:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# And within the DataFrame API use:\n",
    "# data is stored at external path and not in spark data warehouse\n",
    "\n",
    "(flights_df\n",
    ".write\n",
    ".option(\"path\", \"/Users/pvasud669@apac.comcast.com/repos/learnings/spark/datasets/unmanaged-tables\")\n",
    ".mode(\"overwrite\")\n",
    ".saveAsTable(\"unmanaged_delay_flights_tbl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28816509-6b82-4c8f-9e0f-b387568ff47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01201755|    0|     449|   ORF|        ATL|\n",
      "|01201610|   52|     449|   ORF|        ATL|\n",
      "|01201441|    0|     449|   ORF|        ATL|\n",
      "|01211755|  -15|     449|   ORF|        ATL|\n",
      "|01210941|   -5|     449|   ORF|        ATL|\n",
      "|01210700|    5|     449|   ORF|        ATL|\n",
      "|01211243|    2|     449|   ORF|        ATL|\n",
      "|01210540|   -5|     449|   ORF|        ATL|\n",
      "|01211610|   27|     449|   ORF|        ATL|\n",
      "|01211441|   -6|     449|   ORF|        ATL|\n",
      "|01221755|   -4|     449|   ORF|        ATL|\n",
      "|01220941|   -5|     449|   ORF|        ATL|\n",
      "|01220700|   -2|     449|   ORF|        ATL|\n",
      "|01221243|    0|     449|   ORF|        ATL|\n",
      "|01220540|   -5|     449|   ORF|        ATL|\n",
      "|01221610|   -5|     449|   ORF|        ATL|\n",
      "|01221441|   -2|     449|   ORF|        ATL|\n",
      "|01231755|   -1|     449|   ORF|        ATL|\n",
      "|01230941|   -8|     449|   ORF|        ATL|\n",
      "|01230700|   -3|     449|   ORF|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# querying from unmanaged table\n",
    "\n",
    "spark.sql(\"select * from unmanaged_delay_flights_tbl;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecce3a-64b8-42e3-a8d5-0b0dae359a28",
   "metadata": {},
   "source": [
    "# Creating Views\n",
    "In addition to creating tables, Spark can create views on top of existing tables. Views\n",
    "can be global (visible across all SparkSessions on a given cluster) or session-scoped\n",
    "(visible only to a single SparkSession), and they are temporary: they disappear after\n",
    "your Spark application terminates.\n",
    "Creating views has a similar syntax to creating tables within a database. Once you create\n",
    "a view, you can query it as you would a table. The difference between a view and a\n",
    "table is that views don’t actually hold the data; tables persist after your Spark application\n",
    "terminates, but views disappear.\n",
    "You can create a view from an existing table using SQL. For example, if you wish to\n",
    "work on only the subset of the US flight delays data set with origin airports of New\n",
    "York (JFK) and San Francisco (SFO), the following queries will create global temporary\n",
    "and temporary views consisting of just that slice of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea666116-2625-4caa-a910-8da2e6ee39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- In SQL\n",
    "\n",
    "# CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    "# SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    "# origin = 'SFO';\n",
    "\n",
    "# CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    "# SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    "# origin = 'JFK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bca5d6-e0b4-48ca-bc76-0d41a2942da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can accomplish the same thing with the DataFrame API as follows:\n",
    "# In Python\n",
    "\n",
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "\n",
    "# Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ed114-53c0-4717-aa79-9c8758dd01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Once you’ve created these views, you can issue queries against them just as you would\n",
    "# # against a table. Keep in mind that when accessing a global temporary view you must\n",
    "# # use the prefix global_temp.<view_name>, because Spark creates global temporary\n",
    "# # views in a global temporary database called global_temp. For example:\n",
    "# # -- In SQL\n",
    "\n",
    "# SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4e57d-bfdd-4572-9ef1-5425eeb6fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By contrast, you can access the normal temporary view without the global_temp\n",
    "# prefix:\n",
    "# -- In SQL\n",
    "# SELECT * FROM us_origin_airport_JFK_tmp_view\n",
    "\n",
    "# // In Scala/Python\n",
    "# spark.read.table(\"us_origin_airport_JFK_tmp_view\")\n",
    "\n",
    "# // Or\n",
    "# spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")\n",
    "\n",
    "# You can also drop a view just like you would a table:\n",
    "# -- In SQL\n",
    "# DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view;\n",
    "# DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view\n",
    "\n",
    "# // In Scala/Python\n",
    "# spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "# spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab844cf6-0af4-4184-9266-4fea1ef4c634",
   "metadata": {},
   "source": [
    "# Viewing the Metadata\n",
    "As mentioned previously, Spark manages the metadata associated with each managed\n",
    "or unmanaged table. This is captured in the Catalog, a high-level abstraction in\n",
    "Spark SQL for storing metadata. The Catalog’s functionality was expanded in Spark\n",
    "2.x with new public methods enabling you to examine the metadata associated with\n",
    "your databases, tables, and views. Spark 3.0 extends it to use external catalog (which\n",
    "we briefly discuss in Chapter 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4d96dd9-e5b8-48b3-97b9-45b5a7e25ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/Users/pvasud669@apac.comcast.com/repos/learnings/spark/spark-warehouse'),\n",
       " Database(name='learn_spark_db', catalog='spark_catalog', description='', locationUri='file:/Users/pvasud669@apac.comcast.com/repos/learnings/spark/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='managed_us_delay_flights_tbl_new', catalog='spark_catalog', namespace=['learn_spark_db'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='unmanaged_delay_flights_tbl', catalog='spark_catalog', namespace=['learn_spark_db'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='flight_delay_view', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example, within a Spark application, after creating the SparkSession variable\n",
    "# spark, you can access all the stored metadata through methods like these:\n",
    "\n",
    "# %config InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# // In Scala/Python\n",
    "(spark.catalog.listDatabases())\n",
    "(spark.catalog.listTables())\n",
    "(spark.catalog.listColumns(\"managed_us_delay_flights_tbl_new\"))\n",
    "(spark.catalog.listColumns(\"unmanaged_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec200502-4048-45d3-ad7a-20dafe303b56",
   "metadata": {},
   "source": [
    "# Caching SQL Tables\n",
    "Although we will discuss table caching strategies in the next chapter, it’s worth mentioning\n",
    "here that, like DataFrames, you can cache and uncache SQL tables and views.\n",
    "In Spark 3.0, in addition to other options, you can specify a table as LAZY, meaning\n",
    "that it should only be cached when it is first used instead of immediately:\n",
    "\n",
    "-- In SQL\n",
    "CACHE [LAZY] TABLE <table-name>\n",
    "UNCACHE TABLE <table-name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99fa938-3554-4741-bcfd-2134a89e979f",
   "metadata": {},
   "source": [
    "# Reading Tables into DataFrames\n",
    "Often, data engineers build data pipelines as part of their regular data ingestion and\n",
    "ETL processes. They populate Spark SQL databases and tables with cleansed data for\n",
    "consumption by applications downstream.\n",
    "Let’s assume you have an existing database, learn_spark_db, and table,\n",
    "us_delay_flights_tbl, ready for use. Instead of reading from an external JSON file,\n",
    "you can simply use SQL to query the table and assign the returned result to a\n",
    "DataFrame:\n",
    "\n",
    "# In Python\n",
    "us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")\n",
    "\n",
    "Now you have a cleansed DataFrame read from an existing Spark SQL table. You can\n",
    "also read data in other formats using Spark’s built-in data sources, giving you the flexibility\n",
    "to interact with various common file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80f540-2ef4-42c7-9bb4-9d01789c2b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c31a0-e9d1-4817-8dd5-588b4927b8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
